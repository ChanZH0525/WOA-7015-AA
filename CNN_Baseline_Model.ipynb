{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanZH0525/WOA-7015-AA/blob/main/CNN_Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnYQfvuKJl4I"
      },
      "source": [
        "# VQA-RAD Baseline: VGG16 + LSTM + Stacked Attention Network\n",
        "\n",
        "**Architecture:**\n",
        "- Image Encoder: VGG16 (pre-trained, frozen)\n",
        "- Question Encoder: LSTM (300-dim embedding, 512 hidden)\n",
        "- Attention: 2-layer Stacked Attention Network\n",
        "- Classifier: dropout 0.3, direct mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgFoJ7LJl4L"
      },
      "source": [
        "---\n",
        "## üì¶ Section 0: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrw47vX_Jl4L"
      },
      "source": [
        "### 0.1 Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHTjzZyeJl4M"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision pillow pandas numpy matplotlib scikit-learn nltk -q\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"‚úì Dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2IwLeHqJl4N"
      },
      "source": [
        "### 0.2 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3Ic_04fJl4N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "print(\"‚úì Libraries imported\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABxEGVLBJl4N"
      },
      "source": [
        "### 0.3 Set Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh8W0yFFJl4N"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"‚úì Seed set to {seed}\")\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01fLdfISJl4O"
      },
      "source": [
        "### 0.4 Configure Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAQWRVjaJl4O"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DEVICE CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU (training will be slower)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsOlAtoxJl4O"
      },
      "source": [
        "### 0.5 Upload Data Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FJD9obzJl4O"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FILE UPLOAD\")\n",
        "print(\"=\"*80)\n",
        "print(\"Upload: VQA_RAD_Preprocessed.csv and VQA_RAD_Image_Folder.zip\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(f\"\\n‚úì Uploaded {len(uploaded)} files\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbGLvEShJl4O"
      },
      "source": [
        "### 0.6 Extract Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWgHbKcKJl4P"
      },
      "outputs": [],
      "source": [
        "print(\"Extracting images...\")\n",
        "\n",
        "with zipfile.ZipFile('VQA_RAD_Image_Folder.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "IMAGE_DIR = 'VQA_RAD_Image_Folder'\n",
        "CSV_FILE = 'VQA_RAD_Preprocessed.csv'\n",
        "\n",
        "print(f\"‚úì Images extracted to {IMAGE_DIR}\")\n",
        "print(f\"‚úì CSV: {CSV_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8lWk5PiJl4P"
      },
      "source": [
        "---\n",
        "## ‚öôÔ∏è Section 1: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkB-AKIKJl4P"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Paths\n",
        "    CSV_PATH = CSV_FILE\n",
        "    IMAGE_DIR = IMAGE_DIR\n",
        "\n",
        "    # Random seed\n",
        "    SEED = 42\n",
        "    DEVICE = device\n",
        "\n",
        "    # Image\n",
        "    IMAGE_SIZE = 224\n",
        "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Question\n",
        "    MAX_QUESTION_LENGTH = 21\n",
        "    EMBEDDING_DIM = 300\n",
        "    LSTM_HIDDEN_SIZE = 512\n",
        "    LSTM_NUM_LAYERS = 1\n",
        "\n",
        "    # Attention\n",
        "    ATTENTION_DIM = 512\n",
        "    NUM_ATTENTION_LAYERS = 2\n",
        "\n",
        "    # Training\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 1e-3\n",
        "    WEIGHT_DECAY = 1e-5  # L2 regularization\n",
        "    NUM_EPOCHS = 100\n",
        "    PATIENCE = 20\n",
        "    TRAIN_VAL_SPLIT = 0.85\n",
        "\n",
        "    # Will be set after building vocab/labels\n",
        "    VOCAB_SIZE = None\n",
        "    NUM_CLASSES = None\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"L2 weight decay: {config.WEIGHT_DECAY}\")\n",
        "print(f\"Max question length: {config.MAX_QUESTION_LENGTH}\")\n",
        "print(f\"Embedding dim: {config.EMBEDDING_DIM}\")\n",
        "print(f\"LSTM hidden: {config.LSTM_HIDDEN_SIZE}\")\n",
        "print(f\"Attention layers: {config.NUM_ATTENTION_LAYERS}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0y7JgvzJl4P"
      },
      "source": [
        "---\n",
        "## üìä Section 2: Data Loading & Preparation\n",
        "\n",
        "1. Load data\n",
        "2. **Partition FIRST** (train/val/test)\n",
        "3. Build vocabulary from TRAINING only\n",
        "4. Build labels from TRAINING only\n",
        "\n",
        "This ensures zero information leakage from test/validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeG45ATHJl4P"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBfbmxlSJl4P"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df = pd.read_csv(config.CSV_PATH)\n",
        "\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nSplit distribution:\")\n",
        "print(df['split'].value_counts())\n",
        "print(f\"\\nUnique answers: {df['answer'].nunique()}\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1egCplcXJl4Q"
      },
      "source": [
        "### 2.2 Data Partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjFaYhSkJl4Q"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"DATA PARTITIONING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Separate test set (already split in dataset)\n",
        "df_test = df[df['split'] == 'Test'].reset_index(drop=True)\n",
        "df_train_full = df[df['split'] == 'Train'].reset_index(drop=True)\n",
        "\n",
        "print(f\"Original split:\")\n",
        "print(f\"  Train (full): {len(df_train_full)}\")\n",
        "print(f\"  Test: {len(df_test)}\")\n",
        "\n",
        "# Split train into train/val (stratified by answer_type, not individual answer)\n",
        "train_idx, val_idx = train_test_split(\n",
        "    range(len(df_train_full)),\n",
        "    test_size=1-config.TRAIN_VAL_SPLIT,\n",
        "    random_state=config.SEED,\n",
        "    stratify=df_train_full['answer_type']  # CLOSED/OPEN\n",
        ")\n",
        "\n",
        "df_train = df_train_full.iloc[train_idx].reset_index(drop=True)\n",
        "df_val = df_train_full.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nFinal split (stratified by answer_type):\")\n",
        "print(f\"  Train: {len(df_train)} ({len(df_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Val: {len(df_val)} ({len(df_val)/len(df)*100:.1f}%)\")\n",
        "print(f\"  Test: {len(df_test)} ({len(df_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Verify stratification\n",
        "print(f\"\\nStratification check:\")\n",
        "for atype in ['CLOSED', 'OPEN']:\n",
        "    train_pct = (df_train['answer_type'] == atype).mean() * 100\n",
        "    val_pct = (df_val['answer_type'] == atype).mean() * 100\n",
        "    print(f\"  {atype}: Train {train_pct:.1f}%, Val {val_pct:.1f}%\")\n",
        "\n",
        "print(\"\\n‚úì Data partitioned\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRy5eKppJl4Q"
      },
      "source": [
        "### 2.3 Build Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTMEBNWhJl4Q"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"BUILDING VOCABULARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def build_vocabulary(questions, min_freq=1):\n",
        "    word_counter = Counter()\n",
        "    for question in questions:\n",
        "        words = str(question).lower().split()\n",
        "        word_counter.update(words)\n",
        "\n",
        "    # Filter by frequency\n",
        "    words = [w for w, c in word_counter.items() if c >= min_freq]\n",
        "    words = sorted(words)\n",
        "\n",
        "    # Create mappings (0=PAD, 1=UNK)\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for idx, word in enumerate(words, start=2):\n",
        "        word2idx[word] = idx\n",
        "\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "    return word2idx, idx2word, word_counter\n",
        "\n",
        "# Build from TRAINING only\n",
        "word2idx, idx2word, word_counter = build_vocabulary(df_train['question'].tolist())\n",
        "\n",
        "config.VOCAB_SIZE = len(word2idx)\n",
        "\n",
        "print(f\"Vocabulary size: {config.VOCAB_SIZE}\")\n",
        "print(f\"  <PAD>: {word2idx['<PAD>']}\")\n",
        "print(f\"  <UNK>: {word2idx['<UNK>']}\")\n",
        "print(f\"  Unique words: {len(word2idx) - 2}\")\n",
        "\n",
        "print(f\"\\nTop 10 words:\")\n",
        "for word, count in word_counter.most_common(10):\n",
        "    print(f\"  {word}: {count}\")\n",
        "\n",
        "print(f\"\\n‚úì Vocabulary built from training: {config.VOCAB_SIZE} words\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pIuWJePJl4R"
      },
      "source": [
        "### 2.4 Build Answer Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkFVwhW5Jl4R"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"BUILDING ANSWER LABELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def build_answer_mapping(answers):\n",
        "    unique_answers = sorted(set(answers))\n",
        "    answer2idx = {ans: idx for idx, ans in enumerate(unique_answers)}\n",
        "    idx2answer = {idx: ans for ans, idx in answer2idx.items()}\n",
        "    return answer2idx, idx2answer\n",
        "\n",
        "# Build from TRAINING only\n",
        "answer2idx, idx2answer = build_answer_mapping(df_train['answer'].tolist())\n",
        "\n",
        "config.NUM_CLASSES = len(answer2idx)\n",
        "\n",
        "# Statistics from TRAINING only\n",
        "answer_counts = df_train['answer'].value_counts()\n",
        "\n",
        "print(f\"Answer classes: {config.NUM_CLASSES}\")\n",
        "print(f\"\\nStatistics (training):\")\n",
        "print(f\"  Most common: '{answer_counts.index[0]}' ({answer_counts.iloc[0]})\")\n",
        "print(f\"  Least common: {(answer_counts == 1).sum()} with 1 sample\")\n",
        "print(f\"  Mean: {answer_counts.mean():.2f}\")\n",
        "\n",
        "print(f\"\\nTop 10 answers:\")\n",
        "for i, (ans, count) in enumerate(answer_counts.head(10).items(), 1):\n",
        "    print(f\"  {i}. '{ans}': {count} (class {answer2idx[ans]})\")\n",
        "\n",
        "print(f\"\\n‚úì Answer labels built from training: {config.NUM_CLASSES} classes\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiKsV1ZUJl4R"
      },
      "source": [
        "---\n",
        "## üóÇÔ∏è Section 3: Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q90C7zH2Jl4V"
      },
      "source": [
        "### 3.1 Image Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7jnNZelJl4W"
      },
      "outputs": [],
      "source": [
        "# Training transform (with augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0, hue=0),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# Val/Test transform (no augmentation)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\n",
        "])\n",
        "\n",
        "print(\"‚úì Transforms defined\")\n",
        "print(\"  Train: Rotation(10¬∞) + ColorJitter(0.15,0.15,0,0)\")\n",
        "print(\"  Val/Test: No augmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emNBDZxNJl4W"
      },
      "source": [
        "### 3.2 VQA Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6WkBeLVJl4W"
      },
      "outputs": [],
      "source": [
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, word2idx, answer2idx, transform, max_length):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.word2idx = word2idx\n",
        "        self.answer2idx = answer2idx\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def encode_question(self, question):\n",
        "        words = str(question).lower().split()\n",
        "        indices = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in words]\n",
        "        actual_length = len(indices)\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indices) < self.max_length:\n",
        "            indices += [self.word2idx['<PAD>']] * (self.max_length - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_length]\n",
        "            actual_length = self.max_length\n",
        "\n",
        "        return torch.LongTensor(indices), actual_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load image\n",
        "        image_path = os.path.join(self.image_dir, row['image_name'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Encode question\n",
        "        question, question_length = self.encode_question(row['question'])\n",
        "\n",
        "        # Handle unseen answers\n",
        "        if row['answer'] in self.answer2idx:\n",
        "            answer_label = self.answer2idx[row['answer']]\n",
        "        else:\n",
        "            answer_label = -1  # Unseen answer\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'question': question,\n",
        "            'question_length': question_length,\n",
        "            'answer': answer_label,\n",
        "            'answer_text': row['answer']\n",
        "        }\n",
        "\n",
        "print(\"‚úì VQADataset class defined\")\n",
        "print(\"  Handles <UNK> words and unseen answers (-1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m57-qhBNJl4W"
      },
      "source": [
        "### 3.3 Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoUtYv9fJl4X"
      },
      "outputs": [],
      "source": [
        "train_dataset = VQADataset(df_train, config.IMAGE_DIR, word2idx, answer2idx,\n",
        "                          train_transform, config.MAX_QUESTION_LENGTH)\n",
        "val_dataset = VQADataset(df_val, config.IMAGE_DIR, word2idx, answer2idx,\n",
        "                        val_test_transform, config.MAX_QUESTION_LENGTH)\n",
        "test_dataset = VQADataset(df_test, config.IMAGE_DIR, word2idx, answer2idx,\n",
        "                         val_test_transform, config.MAX_QUESTION_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATALOADERS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
        "print(f\"Val: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
        "print(f\"Test: {len(test_dataset)} samples, {len(test_loader)} batches\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vNr4qZJl4X"
      },
      "source": [
        "---\n",
        "## üß† Section 4: Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfyzAPhtJl4Y"
      },
      "source": [
        "### 4.1 Image Encoder (VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlnuSDa-Jl4Y"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load pre-trained VGG16\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Extract features up to conv5_3\n",
        "        self.features = nn.Sequential(*list(vgg16.features.children())[:-1])\n",
        "\n",
        "        # Freeze parameters\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        # images: (B, 3, 224, 224)\n",
        "        features = self.features(images)  # (B, 512, 14, 14)\n",
        "\n",
        "        # Reshape for attention: (B, 196, 512)\n",
        "        B, C, H, W = features.shape\n",
        "        features = features.view(B, C, H*W).transpose(1, 2)\n",
        "\n",
        "        return features\n",
        "\n",
        "print(\"‚úì ImageEncoder defined\")\n",
        "print(\"  VGG16 (pre-trained, frozen)\")\n",
        "print(\"  Output: (batch, 196, 512)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoFFQHMQJl4Y"
      },
      "source": [
        "### 4.2 Question Encoder (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P7GztEyJl4Y"
      },
      "outputs": [],
      "source": [
        "class QuestionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
        "                           batch_first=True, bidirectional=False)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, questions, lengths):\n",
        "        # questions: (B, max_len)\n",
        "        # lengths: (B,)\n",
        "\n",
        "        embedded = self.embedding(questions)  # (B, max_len, embedding_dim)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # LSTM\n",
        "        _, (hidden, _) = self.lstm(packed)  # hidden: (num_layers, B, hidden_size)\n",
        "\n",
        "        # Take last layer\n",
        "        question_features = hidden[-1]  # (B, hidden_size)\n",
        "\n",
        "        return question_features\n",
        "\n",
        "print(\"‚úì QuestionEncoder defined\")\n",
        "print(\"  Embedding + LSTM\")\n",
        "print(\"  Output: (batch, 512)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "donoNxiUJl4f"
      },
      "source": [
        "### 4.3 Stacked Attention Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpsCpnQCJl4f"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single attention layer.\n",
        "    Computes attention weights over image regions given question.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dim, question_dim, attention_dim):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        # Project image features\n",
        "        self.image_proj = nn.Linear(image_dim, attention_dim)\n",
        "\n",
        "        # Project question features\n",
        "        self.question_proj = nn.Linear(question_dim, attention_dim)\n",
        "\n",
        "        # Attention scoring\n",
        "        self.attention_score = nn.Linear(attention_dim, 1)\n",
        "\n",
        "    def forward(self, image_features, question_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: (batch, 196, 512)\n",
        "            question_vector: (batch, hidden_size)\n",
        "        Returns:\n",
        "            attended_features: (batch, 512)\n",
        "            attention_weights: (batch, 196)\n",
        "        \"\"\"\n",
        "        # Project image: (batch, 196, attention_dim)\n",
        "        img_proj = self.image_proj(image_features)\n",
        "\n",
        "        # Project question: (batch, attention_dim)\n",
        "        ques_proj = self.question_proj(question_vector)\n",
        "\n",
        "        # Broadcast question to all regions: (batch, 196, attention_dim)\n",
        "        ques_proj = ques_proj.unsqueeze(1).expand_as(img_proj)\n",
        "\n",
        "        # Combined features: (batch, 196, attention_dim)\n",
        "        combined = torch.tanh(img_proj + ques_proj)\n",
        "\n",
        "        # Attention scores: (batch, 196, 1)\n",
        "        scores = self.attention_score(combined)\n",
        "\n",
        "        # Attention weights: (batch, 196)\n",
        "        attention_weights = F.softmax(scores.squeeze(2), dim=1)\n",
        "\n",
        "        # Weighted sum: (batch, 512)\n",
        "        attended_features = torch.bmm(\n",
        "            attention_weights.unsqueeze(1),  # (batch, 1, 196)\n",
        "            image_features  # (batch, 196, 512)\n",
        "        ).squeeze(1)  # (batch, 512)\n",
        "\n",
        "        return attended_features, attention_weights\n",
        "\n",
        "\n",
        "class StackedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Stacked Attention Network with progressive refinement.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dim, question_dim, attention_dim, num_layers=2):\n",
        "        super(StackedAttention, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # First attention layer\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            AttentionLayer(image_dim, question_dim, attention_dim)\n",
        "        ])\n",
        "\n",
        "        # Additional layers (input is question + attended features)\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.attention_layers.append(\n",
        "                AttentionLayer(image_dim, question_dim + image_dim, attention_dim)\n",
        "            )\n",
        "\n",
        "    def forward(self, image_features, question_vector):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: (batch, 196, 512)\n",
        "            question_vector: (batch, hidden_size)\n",
        "        Returns:\n",
        "            final_features: (batch, 512)\n",
        "            all_attention_weights: List of (batch, 196) for each layer\n",
        "        \"\"\"\n",
        "        all_attention_weights = []\n",
        "\n",
        "        # First attention layer\n",
        "        attended, weights = self.attention_layers[0](image_features, question_vector)\n",
        "        all_attention_weights.append(weights)\n",
        "\n",
        "        # Subsequent layers: progressive refinement\n",
        "        for layer in self.attention_layers[1:]:\n",
        "            # Combine question with previous attended features\n",
        "            combined_question = torch.cat([question_vector, attended], dim=1)\n",
        "\n",
        "            # Apply attention with enriched question\n",
        "            attended, weights = layer(image_features, combined_question)\n",
        "            all_attention_weights.append(weights)\n",
        "\n",
        "        return attended, all_attention_weights\n",
        "\n",
        "print(\"‚úì Stacked Attention Network defined\")\n",
        "print(\"  AttentionLayer: Single attention computation\")\n",
        "print(\"  StackedAttention: Progressive refinement across layers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJyKVf1xJl4g"
      },
      "source": [
        "### 4.4 Complete VQA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsS7cMUyJl4g"
      },
      "outputs": [],
      "source": [
        "class VQAModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete VQA model: VGG16 + LSTM + SAN + Classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, config, vocab_size, num_classes):\n",
        "        super(VQAModel, self).__init__()\n",
        "\n",
        "        # Image encoder (VGG16)\n",
        "        self.image_encoder = ImageEncoder()\n",
        "\n",
        "        # Question encoder (LSTM)\n",
        "        self.question_encoder = QuestionEncoder(\n",
        "            vocab_size,\n",
        "            config.EMBEDDING_DIM,\n",
        "            config.LSTM_HIDDEN_SIZE,\n",
        "            config.LSTM_NUM_LAYERS\n",
        "        )\n",
        "\n",
        "        # Stacked attention\n",
        "        self.attention = StackedAttention(\n",
        "            image_dim=512,  # VGG16 conv5_3 channels\n",
        "            question_dim=config.LSTM_HIDDEN_SIZE,\n",
        "            attention_dim=config.ATTENTION_DIM,\n",
        "            num_layers=config.NUM_ATTENTION_LAYERS\n",
        "        )\n",
        "\n",
        "        # Simplified classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512 + config.LSTM_HIDDEN_SIZE, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, questions, question_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: (batch, 3, 224, 224)\n",
        "            questions: (batch, max_length)\n",
        "            question_lengths: (batch)\n",
        "        Returns:\n",
        "            logits: (batch, num_classes)\n",
        "            attention_weights: List of (batch, 196) for each layer\n",
        "        \"\"\"\n",
        "        # Encode image: (batch, 196, 512)\n",
        "        image_features = self.image_encoder(images)\n",
        "\n",
        "        # Encode question: (batch, hidden_size)\n",
        "        question_vector = self.question_encoder(questions, question_lengths)\n",
        "\n",
        "        # Apply attention: (batch, 512), List[(batch, 196)]\n",
        "        attended_features, attention_weights = self.attention(image_features, question_vector)\n",
        "\n",
        "        # Combine\n",
        "        combined = torch.cat([attended_features, question_vector], dim=1)\n",
        "\n",
        "        # Classify\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, attention_weights\n",
        "\n",
        "print(\"‚úì Complete VQA Model defined\")\n",
        "print(\"  Progressive attention refinement\")\n",
        "print(\"  Simplified classifier (dropout 0.3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HDVqm5NJl4g"
      },
      "source": [
        "### 4.5 Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RosLS8rsJl4h"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODEL INITIALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = VQAModel(config, config.VOCAB_SIZE, config.NUM_CLASSES)\n",
        "model = model.to(config.DEVICE)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "frozen_params = total_params - trainable_params\n",
        "\n",
        "print(f\"\\nModel architecture:\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Frozen parameters (VGG16): {frozen_params:,}\")\n",
        "\n",
        "print(f\"\\nModel components:\")\n",
        "print(f\"  Image encoder: VGG16 conv5_3 (frozen)\")\n",
        "print(f\"  Question encoder: Embedding({config.VOCAB_SIZE}, {config.EMBEDDING_DIM}) + LSTM({config.LSTM_HIDDEN_SIZE})\")\n",
        "print(f\"  Attention: {config.NUM_ATTENTION_LAYERS}-layer SAN with progressive refinement\")\n",
        "print(f\"  Classifier: Simplified (dropout 0.3, direct 1024‚Üí{config.NUM_CLASSES})\")\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    sample_images = sample_batch['image'].to(config.DEVICE)\n",
        "    sample_questions = sample_batch['question'].to(config.DEVICE)\n",
        "    sample_lengths = torch.LongTensor(sample_batch['question_length'])\n",
        "\n",
        "    logits, attn_weights = model(sample_images, sample_questions, sample_lengths)\n",
        "\n",
        "    print(f\"\\nTest forward pass:\")\n",
        "    print(f\"  Input images: {sample_images.shape}\")\n",
        "    print(f\"  Input questions: {sample_questions.shape}\")\n",
        "    print(f\"  Output logits: {logits.shape}\")\n",
        "    print(f\"  Attention layers: {len(attn_weights)}\")\n",
        "    print(f\"  Attention weights shape: {attn_weights[0].shape}\")\n",
        "\n",
        "print(\"\\n‚úì Model initialized successfully\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PARAMETER BREAKDOWN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Embedding layer (inside question_encoder)\n",
        "embedding_params = sum(p.numel() for p in model.question_encoder.embedding.parameters())\n",
        "print(f\"Embedding layer: {embedding_params:,}\")\n",
        "\n",
        "# LSTM (inside question_encoder)\n",
        "lstm_params = sum(p.numel() for p in model.question_encoder.lstm.parameters())\n",
        "print(f\"LSTM encoder: {lstm_params:,}\")\n",
        "\n",
        "# Attention mechanism\n",
        "attention_params = sum(p.numel() for p in model.attention.parameters())\n",
        "print(f\"Attention mechanism: {attention_params:,}\")\n",
        "\n",
        "# Classifier\n",
        "classifier_params = sum(p.numel() for p in model.classifier.parameters())\n",
        "print(f\"Classifier: {classifier_params:,}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "total_component = embedding_params + lstm_params + attention_params + classifier_params\n",
        "print(f\"Sum of components: {total_component:,}\")\n",
        "\n",
        "# Total trainable\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable (from model): {trainable:,}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "if total_component == trainable:\n",
        "    print(f\"‚úì Parameter counts match!\")\n",
        "else:\n",
        "    print(f\"‚úó Difference: {trainable - total_component:,}\")"
      ],
      "metadata": {
        "id": "8tJRDslW4zUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üéØ Section 5: Training"
      ],
      "metadata": {
        "id": "QxB9_iscyLF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Compute Class Weights"
      ],
      "metadata": {
        "id": "iHX-95WyySXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING SET ANSWER FREQUENCY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get training set answers\n",
        "train_answers = df_train['answer'].value_counts()\n",
        "\n",
        "print(f\"\\nTotal unique answers in training set: {len(train_answers)}\")\n",
        "print(f\"Total training samples: {len(df_train)}\")\n",
        "\n",
        "print(f\"\\nFrequency statistics:\")\n",
        "print(f\"  Minimum frequency: {train_answers.min()}\")\n",
        "print(f\"  Maximum frequency: {train_answers.max()}\")\n",
        "print(f\"  Mean frequency: {train_answers.mean():.2f}\")\n",
        "print(f\"  Median frequency: {train_answers.median():.1f}\")\n",
        "\n",
        "print(f\"\\n Distribution:\")\n",
        "freq_1 = (train_answers == 1).sum()\n",
        "freq_2_5 = ((train_answers >= 2) & (train_answers <= 5)).sum()\n",
        "freq_6_10 = ((train_answers >= 6) & (train_answers <= 10)).sum()\n",
        "freq_11_plus = (train_answers >= 11).sum()\n",
        "\n",
        "print(f\"  Answers with frequency = 1: {freq_1} ({freq_1/len(train_answers)*100:.1f}%)\")\n",
        "print(f\"  Answers with frequency 2-5: {freq_2_5} ({freq_2_5/len(train_answers)*100:.1f}%)\")\n",
        "print(f\"  Answers with frequency 6-10: {freq_6_10} ({freq_6_10/len(train_answers)*100:.1f}%)\")\n",
        "print(f\"  Answers with frequency 11+: {freq_11_plus} ({freq_11_plus/len(train_answers)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTop 10 most frequent answers:\")\n",
        "for i, (answer, count) in enumerate(train_answers.head(10).items(), 1):\n",
        "    print(f\"  {i}. '{answer}': {count} occurrences ({count/len(df_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nBottom 10 least frequent answers:\")\n",
        "for i, (answer, count) in enumerate(train_answers.tail(10).items(), 1):\n",
        "    print(f\"  {i}. '{answer}': {count} occurrence(s)\")"
      ],
      "metadata": {
        "id": "MY5dMJwnehwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING SET ANSWER FREQUENCY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get training set answers\n",
        "train_answers = df_train['answer'].value_counts()\n",
        "\n",
        "print(f\"\\nTotal unique answers in training set: {len(train_answers)}\")\n",
        "print(f\"Total training samples: {len(df_train)}\")\n",
        "\n",
        "print(f\"\\nFrequency statistics:\")\n",
        "print(f\"  Minimum frequency: {train_answers.min()}\")\n",
        "print(f\"  Maximum frequency: {train_answers.max()}\")\n",
        "print(f\"  Mean frequency: {train_answers.mean():.2f}\")\n",
        "print(f\"  Median frequency: {train_answers.median():.1f}\")\n",
        "\n",
        "print(f\"\\n Distribution:\")\n",
        "freq_1 = (train_answers == 1).sum()\n",
        "freq_2_5 = ((train_answers >= 2) & (train_answers <= 5)).sum()\n",
        "freq_6_10 = ((train_answers >= 6) & (train_answers <= 10)).sum()\n",
        "freq_11_plus = (train_answers >= 11).sum()\n",
        "\n",
        "print(f\"  Answers with frequency = 1: {freq_1} ({freq_1/len(train_answers)*100:.1f}%)\")\n",
        "print(f\"  Answers with frequency 2-5: {freq_2_5} ({freq_2_5/len(train_answers)*100:.1f}%)\")\n",
        "print(f\"  Answers with frequency 6-10: {freq_6_10} ({freq_6_10/len(train_answers)*100:.1f}%)\")\n",
        "print(f\"  Answers with frequency 11+: {freq_11_plus} ({freq_11_plus/len(train_answers)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTop 10 most frequent answers:\")\n",
        "for i, (answer, count) in enumerate(train_answers.head(10).items(), 1):\n",
        "    print(f\"  {i}. '{answer}': {count} occurrences ({count/len(df_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nBottom 10 least frequent answers:\")\n",
        "for i, (answer, count) in enumerate(train_answers.tail(10).items(), 1):\n",
        "    print(f\"  {i}. '{answer}': {count} occurrence(s)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# VISUALIZATION OF TOP 10 (Added to your original logic)\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Select the top 10 data points directly from your train_answers variable\n",
        "top_10_data = train_answers.head(10)\n",
        "\n",
        "# 2. Initialize the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# 3. Create the horizontal bar chart\n",
        "# We use horizontal (y='index') so that long medical words are easy to read\n",
        "ax = sns.barplot(\n",
        "    x=top_10_data.values,\n",
        "    y=top_10_data.index,\n",
        "    hue=top_10_data.index,\n",
        "    palette=\"mako\",\n",
        "    legend=False\n",
        ")\n",
        "\n",
        "# 4. Add the exact count labels to the end of each bar for clarity\n",
        "for i, p in enumerate(ax.patches):\n",
        "    width = p.get_width()\n",
        "    ax.text(\n",
        "        width + 1,\n",
        "        p.get_y() + p.get_height()/2,\n",
        "        f'{int(width)}',\n",
        "        va='center',\n",
        "        fontweight='bold'\n",
        "    )\n",
        "\n",
        "# 5. Set labels based on your analysis headers\n",
        "plt.title('Top 10 Most Frequent Training Set Answers', fontsize=14, pad=15)\n",
        "plt.xlabel('Number of Occurrences')\n",
        "plt.ylabel('Answer Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QUln3t0nspY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qExCpBgzJl4h"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMPUTING CLASS WEIGHTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get training labels\n",
        "train_labels = [answer2idx[ans] for ans in df_train['answer']]\n",
        "\n",
        "# Compute balanced class weights\n",
        "class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights = torch.FloatTensor(class_weights).to(config.DEVICE)\n",
        "\n",
        "print(f\"Classes: {len(class_weights)}\")\n",
        "print(f\"Weight range: [{class_weights.min():.3f}, {class_weights.max():.3f}]\")\n",
        "print(f\"Mean weight: {class_weights.mean():.3f}\")\n",
        "\n",
        "print(\"\\n‚úì Class weights computed from training data\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuyU2O3yJl4h"
      },
      "source": [
        "### 5.2 Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syqxO5n1Jl4h"
      },
      "outputs": [],
      "source": [
        "# Loss function (weighted + ignore unseen answers)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
        "\n",
        "# Optimizer with L2 regularization\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=config.LEARNING_RATE,\n",
        "    weight_decay=config.WEIGHT_DECAY  # L2 regularization\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LOSS & OPTIMIZER\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Loss: Weighted CrossEntropyLoss\")\n",
        "print(f\"  Class weights: balanced\")\n",
        "print(f\"  Ignore index: -1 (unseen answers)\")\n",
        "print(f\"\\nOptimizer: Adam\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"  Weight decay: {config.WEIGHT_DECAY} (L2 regularization)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e5Jzi26Jl4h"
      },
      "source": [
        "### 5.3 Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HtA3NDBJl4i"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        images = batch['image'].to(device)\n",
        "        questions = batch['question'].to(device)\n",
        "        question_lengths = torch.LongTensor(batch['question_length'])\n",
        "        answers = batch['answer'].to(device)\n",
        "\n",
        "        # Forward\n",
        "        logits, _ = model(images, questions, question_lengths)\n",
        "        loss = criterion(logits, answers)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Stats (only for valid answers, not -1)\n",
        "        valid_mask = answers != -1\n",
        "        if valid_mask.sum() > 0:\n",
        "            predictions = logits.argmax(dim=1)\n",
        "            correct += (predictions[valid_mask] == answers[valid_mask]).sum().item()\n",
        "            total += valid_mask.sum().item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            images = batch['image'].to(device)\n",
        "            questions = batch['question'].to(device)\n",
        "            question_lengths = torch.LongTensor(batch['question_length'])\n",
        "            answers = batch['answer'].to(device)\n",
        "\n",
        "            # Forward\n",
        "            logits, _ = model(images, questions, question_lengths)\n",
        "            loss = criterion(logits, answers)\n",
        "\n",
        "            # Stats (only for valid answers)\n",
        "            valid_mask = answers != -1\n",
        "            if valid_mask.sum() > 0:\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                correct += (predictions[valid_mask] == answers[valid_mask]).sum().item()\n",
        "                total += valid_mask.sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "print(\"‚úì Training functions defined\")\n",
        "print(\"  train_one_epoch(), validate()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY0lYa0gJl4i"
      },
      "source": [
        "### 5.4 Training Loop with Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVTJ45A-Jl4i"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(config.NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, criterion, optimizer, config.DEVICE\n",
        "    )\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(\n",
        "        model, val_loader, criterion, config.DEVICE\n",
        "    )\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        print(f\"  ‚úì New best validation accuracy: {best_val_acc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  No improvement ({patience_counter}/{config.PATIENCE})\")\n",
        "\n",
        "    if patience_counter >= config.PATIENCE:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n‚úì Training complete\")\n",
        "print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2R0MZBvJl4j"
      },
      "source": [
        "### 5.5 Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIXSgD1lJl4j"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss\n",
        "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "ax2.axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best Val: {best_val_acc:.4f}')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZAJWSwpJl4j"
      },
      "source": [
        "### 5.6 Load Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U-WU-1OJl4k"
      },
      "outputs": [],
      "source": [
        "# Load best model weights\n",
        "model.load_state_dict(best_model_state)\n",
        "print(f\"‚úì Loaded best model (val acc: {best_val_acc:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45y8GZgRJl4k"
      },
      "source": [
        "---\n",
        "## üìä Section 6: Comprehensive Evaluation\n",
        "\n",
        "**Dual Reporting Strategy:**\n",
        "- **Primary:** Full test set (all 451 samples) - real-world performance\n",
        "- **Reference:** Answerable subset (~333 samples) - model capability analysis\n",
        "\n",
        "Samples with unseen answers are counted as incorrect in full test set metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_J2Ga1sJl4l"
      },
      "source": [
        "### 6.1 Get Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh4GJZ4RJl4m"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TEST SET PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "all_predicted_answers = []\n",
        "all_true_answers = []\n",
        "all_probabilities = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images = batch['image'].to(config.DEVICE)\n",
        "        questions = batch['question'].to(config.DEVICE)\n",
        "        question_lengths = torch.LongTensor(batch['question_length'])\n",
        "        answers = batch['answer'].to(config.DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(images, questions, question_lengths)\n",
        "        predictions = logits.argmax(dim=1)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Store\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_true_labels.extend(answers.cpu().numpy())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "        # Convert to answer text\n",
        "        for pred, true_text in zip(predictions.cpu().numpy(), batch['answer_text']):\n",
        "            all_predicted_answers.append(idx2answer[pred])\n",
        "            all_true_answers.append(true_text)\n",
        "\n",
        "# Convert to arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "# Identify unseen answers\n",
        "unseen_mask = all_true_labels == -1\n",
        "answerable_mask = ~unseen_mask\n",
        "\n",
        "num_unseen = unseen_mask.sum()\n",
        "num_answerable = answerable_mask.sum()\n",
        "\n",
        "print(f\"\\n‚úì Collected predictions for {len(all_predictions)} test samples\")\n",
        "print(f\"   Answerable (seen answers): {num_answerable} ({num_answerable/len(all_predictions)*100:.1f}%)\")\n",
        "print(f\"   Unanswerable (unseen answers): {num_unseen} ({num_unseen/len(all_predictions)*100:.1f}%)\")\n",
        "\n",
        "# For full test set evaluation: convert unseen to impossible class\n",
        "all_true_labels_eval = all_true_labels.copy()\n",
        "all_true_labels_eval[unseen_mask] = -9999  # Will never match predictions\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMEyvdq4Jl4n"
      },
      "source": [
        "### 6.2 Overall Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHlUo7D7Jl4n"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"OVERALL TEST ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Full test set accuracy\n",
        "overall_acc_full = accuracy_score(all_true_labels_eval, all_predictions)\n",
        "correct_full = (all_predictions == all_true_labels_eval).sum()\n",
        "\n",
        "# Answerable subset accuracy\n",
        "overall_acc_answerable = accuracy_score(\n",
        "    all_true_labels_eval[answerable_mask],\n",
        "    all_predictions[answerable_mask]\n",
        ")\n",
        "correct_answerable = (all_predictions[answerable_mask] == all_true_labels_eval[answerable_mask]).sum()\n",
        "\n",
        "print(\"\\n[FULL TEST SET - Primary Metric]\")\n",
        "print(f\"  Total samples: {len(all_predictions)}\")\n",
        "print(f\"  Accuracy: {overall_acc_full:.4f} ({overall_acc_full*100:.2f}%)\")\n",
        "print(f\"  Correct: {correct_full} / {len(all_predictions)}\")\n",
        "\n",
        "print(\"\\n[ANSWERABLE SUBSET - Reference]\")\n",
        "print(f\"  Total samples: {num_answerable}\")\n",
        "print(f\"  Accuracy: {overall_acc_answerable:.4f} ({overall_acc_answerable*100:.2f}%)\")\n",
        "print(f\"  Correct: {correct_answerable} / {num_answerable}\")\n",
        "\n",
        "print(f\"\\nNote: Full test includes {num_unseen} unanswerable samples (counted as incorrect)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMiPbdf8Jl4o"
      },
      "source": [
        "### 6.3 Prediction Distribution Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSyoibCWJl4o"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PREDICTION DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "pred_counts = pd.Series(all_predicted_answers).value_counts()\n",
        "true_counts = pd.Series(all_true_answers).value_counts()\n",
        "\n",
        "print(f\"\\nTop 10 Predicted Answers:\")\n",
        "for i, (ans, count) in enumerate(pred_counts.head(10).items(), 1):\n",
        "    pct = count / len(all_predicted_answers) * 100\n",
        "    print(f\"  {i}. '{ans}': {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTop 10 True Answers:\")\n",
        "for i, (ans, count) in enumerate(true_counts.head(10).items(), 1):\n",
        "    pct = count / len(all_true_answers) * 100\n",
        "    print(f\"  {i}. '{ans}': {count} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass collapse analysis:\")\n",
        "print(f\"  Unique predictions: {len(pred_counts)}\")\n",
        "print(f\"  Unique true answers: {len(true_counts)}\")\n",
        "print(f\"  Coverage: {len(pred_counts) / len(true_counts) * 100:.1f}%\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llfSlHB5Jl4p"
      },
      "source": [
        "### 6.4 Unseen Answers Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHtCMSYUJl4p"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"UNSEEN ANSWERS ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nMethodology Validation:\")\n",
        "print(\"  ‚úÖ We built vocabulary/labels from TRAINING only\")\n",
        "print(\"  ‚úÖ This prevents information leakage\")\n",
        "print(\"  ‚úÖ Some test samples have answers not seen during training\")\n",
        "print(\"  ‚úÖ These samples are counted as incorrect (standard practice)\\n\")\n",
        "\n",
        "train_answers = set(df_train['answer'].unique())\n",
        "test_answers = set(df_test['answer'].unique())\n",
        "\n",
        "unseen_answers = test_answers - train_answers\n",
        "test_with_unseen = df_test[df_test['answer'].isin(unseen_answers)]\n",
        "\n",
        "print(f\"Training answers: {len(train_answers)}\")\n",
        "print(f\"Test answers: {len(test_answers)}\")\n",
        "print(f\"Unseen in test: {len(unseen_answers)}\")\n",
        "\n",
        "print(f\"\\nTest samples with unseen answers: {len(test_with_unseen)} / {len(df_test)} ({len(test_with_unseen)/len(df_test)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nExamples of unseen answers:\")\n",
        "for i, ans in enumerate(sorted(list(unseen_answers))[:5], 1):\n",
        "    count = (df_test['answer'] == ans).sum()\n",
        "    print(f\"  {i}. '{ans}' ({count} samples)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Impact: These {len(test_with_unseen)} samples lower full test accuracy\")\n",
        "print(f\"   This is expected and methodologically correct\")\n",
        "print(f\"   'Partition first' prevents overfitting to test distribution\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A42jkGY4Jl4p"
      },
      "source": [
        "### 6.5 Prediction Confidence Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4RFvv4KJl4p"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get confidence scores\n",
        "confidences = np.array([all_probabilities[i][all_predictions[i]] for i in range(len(all_predictions))])\n",
        "correct_flags = (all_predictions == all_true_labels_eval)\n",
        "\n",
        "# Full test set statistics\n",
        "print(\"\\n[FULL TEST SET]\")\n",
        "print(f\"  Mean confidence: {confidences.mean():.4f}\")\n",
        "print(f\"  Median confidence: {np.median(confidences):.4f}\")\n",
        "print(f\"  Std confidence: {confidences.std():.4f}\")\n",
        "\n",
        "print(f\"\\n  Correct predictions: ({correct_flags.sum()} samples)\")\n",
        "print(f\"    Mean confidence: {confidences[correct_flags].mean():.4f}\")\n",
        "print(f\"  Incorrect predictions: ({(~correct_flags).sum()} samples)\")\n",
        "print(f\"    Mean confidence: {confidences[~correct_flags].mean():.4f}\")\n",
        "\n",
        "print(f\"\\n  Answerable samples: ({answerable_mask.sum()} samples)\")\n",
        "print(f\"    Mean confidence: {confidences[answerable_mask].mean():.4f}\")\n",
        "print(f\"  Unanswerable samples: ({unseen_mask.sum()} samples)\")\n",
        "print(f\"    Mean confidence: {confidences[unseen_mask].mean():.4f}\")\n",
        "\n",
        "# Answerable subset statistics\n",
        "print(\"\\n[ANSWERABLE SUBSET - Reference]\")\n",
        "ans_conf = confidences[answerable_mask]\n",
        "ans_correct = correct_flags[answerable_mask]\n",
        "\n",
        "print(f\"  Mean confidence: {ans_conf.mean():.4f}\")\n",
        "print(f\"  Correct predictions: ({ans_correct.sum()} samples)\")\n",
        "print(f\"    Mean confidence: {ans_conf[ans_correct].mean():.4f}\")\n",
        "print(f\"  Incorrect predictions: ({(~ans_correct).sum()} samples)\")\n",
        "print(f\"    Mean confidence: {ans_conf[~ans_correct].mean():.4f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram\n",
        "ax1.hist(confidences[correct_flags], bins=30, alpha=0.6, label='Correct', color='green')\n",
        "ax1.hist(confidences[~correct_flags], bins=30, alpha=0.6, label='Incorrect', color='red')\n",
        "ax1.set_xlabel('Confidence')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('Confidence Distribution (Full Test Set)')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "ax2.boxplot([\n",
        "    confidences[answerable_mask & correct_flags],\n",
        "    confidences[answerable_mask & ~correct_flags],\n",
        "    confidences[unseen_mask]\n",
        "], labels=['Answerable\\nCorrect', 'Answerable\\nIncorrect', 'Unanswerable\\n(Unseen)'])\n",
        "ax2.set_ylabel('Confidence')\n",
        "ax2.set_title('Confidence by Category')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Model shows lower confidence on unanswerable samples\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6VEM-kCJl4q"
      },
      "source": [
        "### 6.6 Confusion Matrix (Top 10 Classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8m8gISiJl4q"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CONFUSION MATRIX (TOP 10 CLASSES)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get top 10 from full test set\n",
        "test_answer_counts = pd.Series(all_true_answers).value_counts()\n",
        "top_10_answers = test_answer_counts.head(10).index.tolist()\n",
        "\n",
        "# Get class IDs (only for those in answer2idx)\n",
        "top_10_ids = []\n",
        "top_10_labels = []\n",
        "for ans in top_10_answers:\n",
        "    if ans in answer2idx:\n",
        "        top_10_ids.append(answer2idx[ans])\n",
        "        top_10_labels.append(ans)\n",
        "\n",
        "print(f\"\\nTop 10 most frequent classes in test set:\")\n",
        "for i, (ans, idx) in enumerate(zip(top_10_labels, top_10_ids), 1):\n",
        "    count = test_answer_counts[ans]\n",
        "    print(f\"  {i}. '{ans}' (class {idx}): {count} samples\")\n",
        "\n",
        "# Full test set confusion matrix\n",
        "mask_full = np.isin(all_true_labels_eval, top_10_ids)\n",
        "cm_full = confusion_matrix(\n",
        "    all_true_labels_eval[mask_full],\n",
        "    all_predictions[mask_full],\n",
        "    labels=top_10_ids\n",
        ")\n",
        "\n",
        "# Answerable subset confusion matrix\n",
        "mask_ans = np.isin(all_true_labels_eval[answerable_mask], top_10_ids)\n",
        "cm_answerable = confusion_matrix(\n",
        "    all_true_labels_eval[answerable_mask][mask_ans],\n",
        "    all_predictions[answerable_mask][mask_ans],\n",
        "    labels=top_10_ids\n",
        ")\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Full test set\n",
        "sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=[ans[:12] for ans in top_10_labels],\n",
        "            yticklabels=[ans[:12] for ans in top_10_labels],\n",
        "            ax=ax1)\n",
        "ax1.set_xlabel('Prediction')\n",
        "ax1.set_ylabel('Ground Truth')\n",
        "ax1.set_title(f'Full Test Set')\n",
        "\n",
        "# Answerable subset\n",
        "sns.heatmap(cm_answerable, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=[ans[:12] for ans in top_10_labels],\n",
        "            yticklabels=[ans[:12] for ans in top_10_labels],\n",
        "            ax=ax2)\n",
        "ax2.set_xlabel('Prediction')\n",
        "ax2.set_ylabel('Ground Truth')\n",
        "ax2.set_title(f'Answerable Test Subset')\n",
        "\n",
        "plt.suptitle('Confusion Matrix (Top 10 Most Frequent Classes in Test Set)', fontsize=14, y=1.00)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"CONFUSION MATRIX (TOP 10 CLASSES)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get top 10 from full test set\n",
        "test_answer_counts = pd.Series(all_true_answers).value_counts()\n",
        "top_10_answers = test_answer_counts.head(10).index.tolist()\n",
        "\n",
        "# Get class IDs (only for those in answer2idx)\n",
        "top_10_ids = []\n",
        "top_10_labels = []\n",
        "for ans in top_10_answers:\n",
        "    if ans in answer2idx:\n",
        "        top_10_ids.append(answer2idx[ans])\n",
        "        top_10_labels.append(ans)\n",
        "\n",
        "print(f\"\\nTop 10 most frequent classes in test set:\")\n",
        "for i, (ans, idx) in enumerate(zip(top_10_labels, top_10_ids), 1):\n",
        "    count = test_answer_counts[ans]\n",
        "    print(f\"  {i}. '{ans}' (class {idx}): {count} samples\")\n",
        "\n",
        "# Full test set confusion matrix\n",
        "mask_full = np.isin(all_true_labels_eval, top_10_ids)\n",
        "cm_full = confusion_matrix(\n",
        "    all_true_labels_eval[mask_full],\n",
        "    all_predictions[mask_full],\n",
        "    labels=top_10_ids\n",
        ")\n",
        "\n",
        "# Answerable subset confusion matrix\n",
        "mask_ans = np.isin(all_true_labels_eval[answerable_mask], top_10_ids)\n",
        "cm_answerable = confusion_matrix(\n",
        "    all_true_labels_eval[answerable_mask][mask_ans],\n",
        "    all_predictions[answerable_mask][mask_ans],\n",
        "    labels=top_10_ids\n",
        ")\n",
        "\n",
        "# PLOT FULL TEST SET MATRIX\n",
        "print(\"\\nFull Test Set\\n\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm_full,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=[ans[:12] for ans in top_10_labels],\n",
        "    yticklabels=[ans[:12] for ans in top_10_labels]\n",
        ")\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Ground Truth')\n",
        "plt.title(f'Confusion Matrix [Top 10 Most Frequent Classes in Full Test Set (N = 451)]')\n",
        "plt.tight_layout()\n",
        "plt.show()  # Displays the first matrix\n",
        "\n",
        "# PLOT ANSWERABLE SUBSET MATRIX\n",
        "print(\"\\nAnswerable Test Subset\\n\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm_answerable,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Greens',\n",
        "    xticklabels=[ans[:12] for ans in top_10_labels],\n",
        "    yticklabels=[ans[:12] for ans in top_10_labels]\n",
        ")\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Ground Truth')\n",
        "plt.title(f'Confusion Matrix (Top 10 Most Frequent Classes in Test Set)')\n",
        "plt.tight_layout()\n",
        "plt.show()  # Displays the second matrix"
      ],
      "metadata": {
        "id": "fpX8-VnA9n49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2MJ0v3rJl4q"
      },
      "source": [
        "### 6.7 Macro Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwkgEn_hJl4r"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MACRO METRICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Full test set\n",
        "macro_f1_full = f1_score(all_true_labels_eval, all_predictions, average='macro', zero_division=0)\n",
        "macro_precision_full = precision_score(all_true_labels_eval, all_predictions, average='macro', zero_division=0)\n",
        "macro_recall_full = recall_score(all_true_labels_eval, all_predictions, average='macro', zero_division=0)\n",
        "\n",
        "# Answerable subset\n",
        "macro_f1_ans = f1_score(\n",
        "    all_true_labels_eval[answerable_mask],\n",
        "    all_predictions[answerable_mask],\n",
        "    average='macro', zero_division=0\n",
        ")\n",
        "macro_precision_ans = precision_score(\n",
        "    all_true_labels_eval[answerable_mask],\n",
        "    all_predictions[answerable_mask],\n",
        "    average='macro', zero_division=0\n",
        ")\n",
        "macro_recall_ans = recall_score(\n",
        "    all_true_labels_eval[answerable_mask],\n",
        "    all_predictions[answerable_mask],\n",
        "    average='macro', zero_division=0\n",
        ")\n",
        "\n",
        "print(\"\\n[FULL TEST SET]\")\n",
        "print(f\"  Macro F1-Score: {macro_f1_full:.4f}\")\n",
        "print(f\"  Macro Precision: {macro_precision_full:.4f}\")\n",
        "print(f\"  Macro Recall: {macro_recall_full:.4f}\")\n",
        "\n",
        "print(\"\\n[ANSWERABLE SUBSET - Reference]\")\n",
        "print(f\"  Macro F1-Score: {macro_f1_ans:.4f}\")\n",
        "print(f\"  Macro Precision: {macro_precision_ans:.4f}\")\n",
        "print(f\"  Macro Recall: {macro_recall_ans:.4f}\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA97d5joJl4r"
      },
      "source": [
        "### 6.8 Per-Answer-Type Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZoLfk6Jl4r"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PER-ANSWER-TYPE ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create results dataframe (full test set)\n",
        "df_test_results = df_test.copy()\n",
        "df_test_results['predicted'] = all_predicted_answers\n",
        "df_test_results['correct'] = [true == pred for true, pred in zip(all_true_answers, all_predicted_answers)]\n",
        "\n",
        "# Full test set\n",
        "answer_type_acc_full = df_test_results.groupby('answer_type')['correct'].mean()\n",
        "\n",
        "# Answerable subset\n",
        "df_test_answerable = df_test_results[answerable_mask].reset_index(drop=True)\n",
        "answer_type_acc_ans = df_test_answerable.groupby('answer_type')['correct'].mean()\n",
        "\n",
        "print(\"\\n[FULL TEST SET]\")\n",
        "for atype, acc in answer_type_acc_full.sort_values(ascending=False).items():\n",
        "    count = (df_test_results['answer_type'] == atype).sum()\n",
        "    print(f\"  {atype}: {acc:.4f} ({count} samples)\")\n",
        "\n",
        "print(\"\\n[ANSWERABLE SUBSET - Reference]\")\n",
        "for atype, acc in answer_type_acc_ans.sort_values(ascending=False).items():\n",
        "    count = (df_test_answerable['answer_type'] == atype).sum()\n",
        "    print(f\"  {atype}: {acc:.4f} ({count} samples)\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtoHlDArJl4r"
      },
      "source": [
        "### 6.9 Per-Question-Type Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZINGM6NuJl4r"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PER-QUESTION-TYPE ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Full test set\n",
        "question_type_acc_full = df_test_results.groupby('question_type')['correct'].mean()\n",
        "\n",
        "# Answerable subset\n",
        "question_type_acc_ans = df_test_answerable.groupby('question_type')['correct'].mean()\n",
        "\n",
        "print(\"\\n[FULL TEST SET]\")\n",
        "for qtype, acc in question_type_acc_full.sort_values(ascending=False).items():\n",
        "    count = (df_test_results['question_type'] == qtype).sum()\n",
        "    print(f\"  {qtype}: {acc:.4f} ({count} samples)\")\n",
        "\n",
        "print(\"\\n[ANSWERABLE SUBSET - Reference]\")\n",
        "for qtype, acc in question_type_acc_ans.sort_values(ascending=False).items():\n",
        "    count = (df_test_answerable['question_type'] == qtype).sum()\n",
        "    print(f\"  {qtype}: {acc:.4f} ({count} samples)\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbaGWDmwJl4s"
      },
      "source": [
        "### 6.10 Per-Image-Organ Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-icPudDJl4t"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PER-IMAGE-ORGAN ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Full test set\n",
        "organ_acc_full = df_test_results.groupby('image_organ')['correct'].mean()\n",
        "\n",
        "# Answerable subset\n",
        "organ_acc_ans = df_test_answerable.groupby('image_organ')['correct'].mean()\n",
        "\n",
        "print(\"\\n[FULL TEST SET]\")\n",
        "for organ, acc in organ_acc_full.sort_values(ascending=False).items():\n",
        "    count = (df_test_results['image_organ'] == organ).sum()\n",
        "    print(f\"  {organ}: {acc:.4f} ({count} samples)\")\n",
        "\n",
        "print(\"\\n[ANSWERABLE SUBSET - Reference]\")\n",
        "for organ, acc in organ_acc_ans.sort_values(ascending=False).items():\n",
        "    count = (df_test_answerable['image_organ'] == organ).sum()\n",
        "    print(f\"  {organ}: {acc:.4f} ({count} samples)\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo3P9koJJl4t"
      },
      "source": [
        "### 6.11 BLEU Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1RsI46MJl4t"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"BLEU SCORES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def compute_bleu_scores(references, hypotheses):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = {}\n",
        "\n",
        "    for n in range(1, 5):\n",
        "        weights = tuple([1/n] * n + [0] * (4-n))\n",
        "        scores = []\n",
        "        for ref, hyp in zip(references, hypotheses):\n",
        "            ref_tokens = [ref.lower().split()]\n",
        "            hyp_tokens = hyp.lower().split()\n",
        "            score = sentence_bleu(ref_tokens, hyp_tokens, weights=weights, smoothing_function=smoothing)\n",
        "            scores.append(score)\n",
        "        bleu_scores[f'BLEU-{n}'] = np.mean(scores)\n",
        "\n",
        "    return bleu_scores\n",
        "\n",
        "# Get answer types for all test samples (matches order of all_true_answers)\n",
        "test_answer_types = df_test['answer_type'].values\n",
        "\n",
        "# Create masks for CLOSED and OPEN\n",
        "closed_mask_full = test_answer_types == 'CLOSED'\n",
        "open_mask_full = test_answer_types == 'OPEN'\n",
        "\n",
        "# Combine with answerable mask for answerable subsets\n",
        "closed_mask_ans = answerable_mask & closed_mask_full\n",
        "open_mask_ans = answerable_mask & open_mask_full\n",
        "\n",
        "# ============================================================================\n",
        "# FULL TEST SET\n",
        "# ============================================================================\n",
        "\n",
        "# All samples\n",
        "bleu_scores_full_all = compute_bleu_scores(all_true_answers, all_predicted_answers)\n",
        "\n",
        "# CLOSED only\n",
        "true_ans_full_closed = [ans for ans, mask in zip(all_true_answers, closed_mask_full) if mask]\n",
        "pred_ans_full_closed = [ans for ans, mask in zip(all_predicted_answers, closed_mask_full) if mask]\n",
        "bleu_scores_full_closed = compute_bleu_scores(true_ans_full_closed, pred_ans_full_closed)\n",
        "\n",
        "# OPEN only\n",
        "true_ans_full_open = [ans for ans, mask in zip(all_true_answers, open_mask_full) if mask]\n",
        "pred_ans_full_open = [ans for ans, mask in zip(all_predicted_answers, open_mask_full) if mask]\n",
        "bleu_scores_full_open = compute_bleu_scores(true_ans_full_open, pred_ans_full_open)\n",
        "\n",
        "# ============================================================================\n",
        "# ANSWERABLE SUBSET\n",
        "# ============================================================================\n",
        "\n",
        "# All answerable\n",
        "true_ans_answerable_all = [ans for ans, mask in zip(all_true_answers, answerable_mask) if mask]\n",
        "pred_ans_answerable_all = [ans for ans, mask in zip(all_predicted_answers, answerable_mask) if mask]\n",
        "bleu_scores_ans_all = compute_bleu_scores(true_ans_answerable_all, pred_ans_answerable_all)\n",
        "\n",
        "# CLOSED answerable only\n",
        "true_ans_ans_closed = [ans for ans, mask in zip(all_true_answers, closed_mask_ans) if mask]\n",
        "pred_ans_ans_closed = [ans for ans, mask in zip(all_predicted_answers, closed_mask_ans) if mask]\n",
        "bleu_scores_ans_closed = compute_bleu_scores(true_ans_ans_closed, pred_ans_ans_closed)\n",
        "\n",
        "# OPEN answerable only\n",
        "true_ans_ans_open = [ans for ans, mask in zip(all_true_answers, open_mask_ans) if mask]\n",
        "pred_ans_ans_open = [ans for ans, mask in zip(all_predicted_answers, open_mask_ans) if mask]\n",
        "bleu_scores_ans_open = compute_bleu_scores(true_ans_ans_open, pred_ans_ans_open)\n",
        "\n",
        "# ============================================================================\n",
        "# DISPLAY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[ALL] (N={len(all_true_answers)})\")\n",
        "for metric, score in bleu_scores_full_all.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[CLOSED ONLY] (N={len(true_ans_full_closed)})\")\n",
        "for metric, score in bleu_scores_full_closed.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[OPEN ONLY] (N={len(true_ans_full_open)})\")\n",
        "for metric, score in bleu_scores_full_open.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANSWERABLE SUBSET - Reference\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[ALL ANSWERABLE] (N={len(true_ans_answerable_all)})\")\n",
        "for metric, score in bleu_scores_ans_all.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[CLOSED ANSWERABLE] (N={len(true_ans_ans_closed)})\")\n",
        "for metric, score in bleu_scores_ans_closed.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[OPEN ANSWERABLE] (N={len(true_ans_ans_open)})\")\n",
        "for metric, score in bleu_scores_ans_open.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrETmF-HJl4t"
      },
      "source": [
        "### 6.12 Wu-Palmer Similarity Score (WBSS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk0m0G2jJl4u"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"WU-PALMER SIMILARITY SCORE (WBSS)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def wu_palmer_similarity(true_ans, pred_ans):\n",
        "    try:\n",
        "        true_synsets = wn.synsets(true_ans.replace(' ', '_'))\n",
        "        pred_synsets = wn.synsets(pred_ans.replace(' ', '_'))\n",
        "\n",
        "        if not true_synsets or not pred_synsets:\n",
        "            return 0.0\n",
        "\n",
        "        max_sim = 0\n",
        "        for ts in true_synsets:\n",
        "            for ps in pred_synsets:\n",
        "                sim = ts.wup_similarity(ps)\n",
        "                if sim and sim > max_sim:\n",
        "                    max_sim = sim\n",
        "        return max_sim\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def compute_wbss(true_answers, pred_answers, accuracy):\n",
        "    \"\"\"Compute WBSS scores\"\"\"\n",
        "    wbss_scores = []\n",
        "    for true_ans, pred_ans in zip(true_answers, pred_answers):\n",
        "        if true_ans == pred_ans:\n",
        "            wbss_scores.append(1.0)\n",
        "        else:\n",
        "            sim = wu_palmer_similarity(true_ans, pred_ans)\n",
        "            wbss_scores.append(sim)\n",
        "\n",
        "    mean_wbss = np.mean(wbss_scores)\n",
        "    median_wbss = np.median(wbss_scores)\n",
        "    semantic_gain = (mean_wbss - accuracy) * 100\n",
        "\n",
        "    return mean_wbss, median_wbss, semantic_gain\n",
        "\n",
        "# Calculate accuracies for each category\n",
        "acc_full_all = overall_acc_full\n",
        "acc_full_closed = sum([t == p for t, p in zip(true_ans_full_closed, pred_ans_full_closed)]) / len(true_ans_full_closed)\n",
        "acc_full_open = sum([t == p for t, p in zip(true_ans_full_open, pred_ans_full_open)]) / len(true_ans_full_open)\n",
        "\n",
        "acc_ans_all = overall_acc_answerable\n",
        "acc_ans_closed = sum([t == p for t, p in zip(true_ans_ans_closed, pred_ans_ans_closed)]) / len(true_ans_ans_closed)\n",
        "acc_ans_open = sum([t == p for t, p in zip(true_ans_ans_open, pred_ans_ans_open)]) / len(true_ans_ans_open)\n",
        "\n",
        "# ============================================================================\n",
        "# COMPUTE WBSS\n",
        "# ============================================================================\n",
        "\n",
        "# FULL TEST SET\n",
        "mean_wbss_full_all, median_wbss_full_all, gain_full_all = compute_wbss(\n",
        "    all_true_answers, all_predicted_answers, acc_full_all\n",
        ")\n",
        "\n",
        "mean_wbss_full_closed, median_wbss_full_closed, gain_full_closed = compute_wbss(\n",
        "    true_ans_full_closed, pred_ans_full_closed, acc_full_closed\n",
        ")\n",
        "\n",
        "mean_wbss_full_open, median_wbss_full_open, gain_full_open = compute_wbss(\n",
        "    true_ans_full_open, pred_ans_full_open, acc_full_open\n",
        ")\n",
        "\n",
        "# ANSWERABLE SUBSET\n",
        "mean_wbss_ans_all, median_wbss_ans_all, gain_ans_all = compute_wbss(\n",
        "    true_ans_answerable_all, pred_ans_answerable_all, acc_ans_all\n",
        ")\n",
        "\n",
        "mean_wbss_ans_closed, median_wbss_ans_closed, gain_ans_closed = compute_wbss(\n",
        "    true_ans_ans_closed, pred_ans_ans_closed, acc_ans_closed\n",
        ")\n",
        "\n",
        "mean_wbss_ans_open, median_wbss_ans_open, gain_ans_open = compute_wbss(\n",
        "    true_ans_ans_open, pred_ans_ans_open, acc_ans_open\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# DISPLAY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[ALL] (N={len(all_true_answers)})\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_full_all:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_full_all:.4f}\")\n",
        "print(f\"  Exact match: {acc_full_all:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_full_all:.1f}%\")\n",
        "\n",
        "print(f\"\\n[CLOSED ONLY] (N={len(true_ans_full_closed)})\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_full_closed:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_full_closed:.4f}\")\n",
        "print(f\"  Exact match: {acc_full_closed:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_full_closed:.1f}%\")\n",
        "\n",
        "print(f\"\\n[OPEN ONLY] (N={len(true_ans_full_open)})\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_full_open:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_full_open:.4f}\")\n",
        "print(f\"  Exact match: {acc_full_open:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_full_open:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANSWERABLE SUBSET - Reference\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[ALL ANSWERABLE] (N={len(true_ans_answerable_all)})\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_ans_all:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_ans_all:.4f}\")\n",
        "print(f\"  Exact match: {acc_ans_all:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_ans_all:.1f}%\")\n",
        "\n",
        "print(f\"\\n[CLOSED ANSWERABLE] (N={len(true_ans_ans_closed)})\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_ans_closed:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_ans_closed:.4f}\")\n",
        "print(f\"  Exact match: {acc_ans_closed:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_ans_closed:.1f}%\")\n",
        "\n",
        "print(f\"\\n[OPEN ANSWERABLE] (N={len(true_ans_ans_open)})\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_ans_open:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_ans_open:.4f}\")\n",
        "print(f\"  Exact match: {acc_ans_open:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_ans_open:.1f}%\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GGo6gTZJl4u"
      },
      "source": [
        "### 6.13 Comprehensive Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqtmWMHHJl4u"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE EVALUATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n[TEST SET COMPOSITION]\")\n",
        "print(f\"  Total test samples: {len(df_test)}\")\n",
        "print(f\"  CLOSED-ended: {closed_mask_full.sum()} ({closed_mask_full.sum()/len(df_test)*100:.1f}%)\")\n",
        "print(f\"  OPEN-ended: {open_mask_full.sum()} ({open_mask_full.sum()/len(df_test)*100:.1f}%)\")\n",
        "print(f\"  Answerable (seen answers): {num_answerable} ({num_answerable/len(df_test)*100:.1f}%)\")\n",
        "print(f\"  Unanswerable (unseen): {num_unseen} ({num_unseen/len(df_test)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# FULL TEST SET - ALL\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL TEST SET - ALL (CLOSED + OPEN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OVERALL] (N={len(all_true_answers)})\")\n",
        "print(f\"  Accuracy: {overall_acc_full:.4f} ({correct_full}/{len(df_test)} correct)\")\n",
        "\n",
        "print(f\"\\n[MACRO METRICS]\")\n",
        "print(f\"  F1-Score: {macro_f1_full:.4f}\")\n",
        "print(f\"  Precision: {macro_precision_full:.4f}\")\n",
        "print(f\"  Recall: {macro_recall_full:.4f}\")\n",
        "\n",
        "print(f\"\\n[BLEU SCORES]\")\n",
        "for metric, score in bleu_scores_full_all.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[SEMANTIC SIMILARITY]\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_full_all:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_full_all:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_full_all:.1f}%\")\n",
        "\n",
        "print(f\"\\n[PER-CATEGORY ACCURACY]\")\n",
        "print(\"  Answer Type:\")\n",
        "for atype, acc in answer_type_acc_full.sort_values(ascending=False).items():\n",
        "    print(f\"    {atype}: {acc:.4f}\")\n",
        "\n",
        "print(\"\\n  Question Type (ALL):\")\n",
        "for qtype, acc in question_type_acc_full.sort_values(ascending=False).items():\n",
        "    print(f\"    {qtype}: {acc:.4f}\")\n",
        "\n",
        "print(\"\\n  Image Organ:\")\n",
        "for organ, acc in organ_acc_full.sort_values(ascending=False).items():\n",
        "    print(f\"    {organ}: {acc:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FULL TEST SET - CLOSED ONLY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL TEST SET - CLOSED-ENDED ONLY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OVERALL] (N={len(true_ans_full_closed)})\")\n",
        "print(f\"  Accuracy: {acc_full_closed:.4f}\")\n",
        "\n",
        "print(f\"\\n[BLEU SCORES]\")\n",
        "for metric, score in bleu_scores_full_closed.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[SEMANTIC SIMILARITY]\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_full_closed:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_full_closed:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_full_closed:.1f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# FULL TEST SET - OPEN ONLY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL TEST SET - OPEN-ENDED ONLY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OVERALL] (N={len(true_ans_full_open)})\")\n",
        "print(f\"  Accuracy: {acc_full_open:.4f}\")\n",
        "\n",
        "print(f\"\\n[BLEU SCORES]\")\n",
        "for metric, score in bleu_scores_full_open.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[SEMANTIC SIMILARITY]\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_full_open:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_full_open:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_full_open:.1f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# ANSWERABLE SUBSET - ALL\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANSWERABLE SUBSET - ALL (Reference)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OVERALL] (N={len(true_ans_answerable_all)})\")\n",
        "print(f\"  Accuracy: {overall_acc_answerable:.4f} ({correct_answerable}/{num_answerable} correct)\")\n",
        "\n",
        "print(f\"\\n[MACRO METRICS]\")\n",
        "print(f\"  F1-Score: {macro_f1_ans:.4f}\")\n",
        "print(f\"  Precision: {macro_precision_ans:.4f}\")\n",
        "print(f\"  Recall: {macro_recall_ans:.4f}\")\n",
        "\n",
        "print(f\"\\n[BLEU SCORES]\")\n",
        "for metric, score in bleu_scores_ans_all.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[SEMANTIC SIMILARITY]\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_ans_all:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_ans_all:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_ans_all:.1f}%\")\n",
        "\n",
        "print(f\"\\n[PER-CATEGORY ACCURACY]\")\n",
        "print(\"  Answer Type:\")\n",
        "for atype, acc in answer_type_acc_ans.sort_values(ascending=False).items():\n",
        "    print(f\"    {atype}: {acc:.4f}\")\n",
        "\n",
        "print(\"\\n  Question Type (ALL):\")\n",
        "for qtype, acc in question_type_acc_ans.sort_values(ascending=False).items():\n",
        "    print(f\"    {qtype}: {acc:.4f}\")\n",
        "\n",
        "print(\"\\n  Image Organ:\")\n",
        "for organ, acc in organ_acc_ans.sort_values(ascending=False).items():\n",
        "    print(f\"    {organ}: {acc:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# ANSWERABLE SUBSET - CLOSED ONLY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANSWERABLE SUBSET - CLOSED-ENDED ONLY (Reference)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OVERALL] (N={len(true_ans_ans_closed)})\")\n",
        "print(f\"  Accuracy: {acc_ans_closed:.4f}\")\n",
        "\n",
        "print(f\"\\n[BLEU SCORES]\")\n",
        "for metric, score in bleu_scores_ans_closed.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[SEMANTIC SIMILARITY]\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_ans_closed:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_ans_closed:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_ans_closed:.1f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# ANSWERABLE SUBSET - OPEN ONLY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANSWERABLE SUBSET - OPEN-ENDED ONLY (Reference)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n[OVERALL] (N={len(true_ans_ans_open)})\")\n",
        "print(f\"  Accuracy: {acc_ans_open:.4f}\")\n",
        "\n",
        "print(f\"\\n[BLEU SCORES]\")\n",
        "for metric, score in bleu_scores_ans_open.items():\n",
        "    print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\n[SEMANTIC SIMILARITY]\")\n",
        "print(f\"  Mean WBSS: {mean_wbss_ans_open:.4f}\")\n",
        "print(f\"  Median WBSS: {median_wbss_ans_open:.4f}\")\n",
        "print(f\"  Semantic gain: +{gain_ans_open:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN6_DC7ZJl4v"
      },
      "source": [
        "---\n",
        "## üîç Section 7: Inference & Visualization\n",
        "\n",
        "**Purpose:** Test the model on sample images and visualize attention maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HICCZWsJl4v"
      },
      "source": [
        "### 7.1 Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IwLOax8Jl4w"
      },
      "outputs": [],
      "source": [
        "def predict_with_attention(model, image_path, question_text, word2idx, idx2answer, transform, device, max_length=23):\n",
        "    \"\"\"\n",
        "    Predict answer and get attention weights for visualization.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load and transform image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
        "\n",
        "    # Encode question\n",
        "    words = question_text.lower().split()\n",
        "    indices = [word2idx.get(word, word2idx['<UNK>']) for word in words]\n",
        "    actual_length = len(indices)\n",
        "\n",
        "    # Pad\n",
        "    if len(indices) < max_length:\n",
        "        indices += [word2idx['<PAD>']] * (max_length - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_length]\n",
        "        actual_length = max_length\n",
        "\n",
        "    question_tensor = torch.LongTensor([indices]).to(device)  # (1, max_length)\n",
        "    question_length = torch.LongTensor([actual_length])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        logits, attention_weights = model(image_tensor, question_tensor, question_length)\n",
        "        prediction = logits.argmax(dim=1).item()\n",
        "        predicted_answer = idx2answer[prediction]\n",
        "\n",
        "        # Get top-5 predictions\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        top5_probs, top5_indices = torch.topk(probs, 5)\n",
        "        top5_answers = [(idx2answer[idx.item()], prob.item())\n",
        "                        for idx, prob in zip(top5_indices[0], top5_probs[0])]\n",
        "\n",
        "    return {\n",
        "        'predicted_answer': predicted_answer,\n",
        "        'top5_answers': top5_answers,\n",
        "        'attention_weights': attention_weights,  # List of attention maps for each layer\n",
        "        'image': image\n",
        "    }\n",
        "\n",
        "print(\"‚úì Inference function defined\")\n",
        "print(\"  Returns: predicted answer, top-5, attention weights (list), image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTOeBfBcJl4w"
      },
      "source": [
        "### 7.2 Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdsniVxpJl4w"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(image, attention_weights, question, predicted_answer, true_answer=None):\n",
        "    \"\"\"\n",
        "    Visualize attention heatmap overlaid on image.\n",
        "    Uses last attention layer (most refined).\n",
        "    \"\"\"\n",
        "    # Get last attention layer weights (most refined)\n",
        "    attn = attention_weights[-1][0].cpu().numpy()  # (196,)\n",
        "\n",
        "    # Reshape to 14x14 spatial grid\n",
        "    attn_map = attn.reshape(14, 14)\n",
        "\n",
        "    # Resize to image size\n",
        "    attn_map_resized = np.array(Image.fromarray(attn_map).resize(\n",
        "        image.size, Image.BILINEAR\n",
        "    ))\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Original Image', fontsize=14)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Attention heatmap\n",
        "    axes[1].imshow(attn_map, cmap='hot', interpolation='bilinear')\n",
        "    axes[1].set_title('Attention Heatmap (14√ó14)', fontsize=14)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Overlay\n",
        "    axes[2].imshow(image)\n",
        "    axes[2].imshow(attn_map_resized, cmap='hot', alpha=0.5, interpolation='bilinear')\n",
        "    axes[2].set_title('Attention Overlay', fontsize=14)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Add text\n",
        "    title = f\"Q: {question}\\n\"\n",
        "    title += f\"Predicted: {predicted_answer}\"\n",
        "    if true_answer:\n",
        "        correct = \"‚úì\" if predicted_answer == true_answer else \"‚úó\"\n",
        "        title += f\" | True: {true_answer} {correct}\"\n",
        "\n",
        "    fig.suptitle(title, fontsize=12, y=0.95)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Visualization function defined\")\n",
        "print(\"  Uses last attention layer for refined visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHBAwcLJJl4w"
      },
      "source": [
        "### 7.3 Sample Predictions with Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3GEydWgJl4w"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SAMPLE PREDICTIONS WITH ATTENTION VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get answerable indices for meaningful visualization\n",
        "answerable_indices = np.where(answerable_mask)[0]\n",
        "\n",
        "# Select samples\n",
        "if len(answerable_indices) >= 40:\n",
        "    sample_positions = [0, 10, 20, 30, 40]\n",
        "    sample_indices = [answerable_indices[pos] for pos in sample_positions if pos < len(answerable_indices)]\n",
        "else:\n",
        "    sample_indices = list(answerable_indices[:min(5, len(answerable_indices))])\n",
        "\n",
        "print(f\"Showing {len(sample_indices)} sample predictions from answerable test samples\\n\")\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    if idx >= len(df_test):\n",
        "        continue\n",
        "\n",
        "    row = df_test.iloc[idx]\n",
        "    image_path = os.path.join(config.IMAGE_DIR, row['image_name'])\n",
        "\n",
        "    # Predict\n",
        "    result = predict_with_attention(\n",
        "        model, image_path, row['question'],\n",
        "        word2idx, idx2answer,\n",
        "        val_test_transform, config.DEVICE\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  Image: {row['image_name']}\")\n",
        "    print(f\"  Question: {row['question']}\")\n",
        "    print(f\"  True answer: {row['answer']}\")\n",
        "    print(f\"  Predicted: {result['predicted_answer']}\")\n",
        "    correct = result['predicted_answer'] == row['answer']\n",
        "    print(f\"  Correct: {correct} {'‚úì' if correct else '‚úó'}\")\n",
        "    print(f\"  Top 5 predictions:\")\n",
        "    for ans, prob in result['top5_answers']:\n",
        "        print(f\"    {ans}: {prob:.4f}\")\n",
        "\n",
        "    # Visualize attention\n",
        "    visualize_attention(\n",
        "        result['image'],\n",
        "        result['attention_weights'],\n",
        "        row['question'],\n",
        "        result['predicted_answer'],\n",
        "        row['answer']\n",
        "    )\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SAMPLE PREDICTIONS WITH ATTENTION VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get answerable indices\n",
        "answerable_indices = np.where(answerable_mask)[0]\n",
        "\n",
        "# Separate by answer type\n",
        "df_test_answerable = df_test.iloc[answerable_indices].reset_index(drop=True)\n",
        "closed_indices = df_test_answerable[df_test_answerable['answer_type'] == 'CLOSED'].index.tolist()\n",
        "open_indices = df_test_answerable[df_test_answerable['answer_type'] == 'OPEN'].index.tolist()\n",
        "\n",
        "# Select 5 from each type\n",
        "num_closed = min(5, len(closed_indices))\n",
        "num_open = min(5, len(open_indices))\n",
        "\n",
        "# Sample evenly across the dataset\n",
        "closed_samples = []\n",
        "if num_closed > 0:\n",
        "    step = len(closed_indices) // num_closed\n",
        "    closed_samples = [closed_indices[i * step] for i in range(num_closed)]\n",
        "\n",
        "open_samples = []\n",
        "if num_open > 0:\n",
        "    step = len(open_indices) // num_open\n",
        "    open_samples = [open_indices[i * step] for i in range(num_open)]\n",
        "\n",
        "# Combine: CLOSED first, then OPEN\n",
        "sample_indices_local = closed_samples + open_samples\n",
        "# Convert local indices (in df_test_answerable) to global indices (in df_test)\n",
        "sample_indices_global = [answerable_indices[idx] for idx in sample_indices_local]\n",
        "\n",
        "print(f\"Showing {len(sample_indices_global)} sample predictions:\")\n",
        "print(f\"  CLOSED answer type: {num_closed} samples\")\n",
        "print(f\"  OPEN answer type: {num_open} samples\")\n",
        "print()\n",
        "\n",
        "for i, idx in enumerate(sample_indices_global, 1):\n",
        "    if idx >= len(df_test):\n",
        "        continue\n",
        "\n",
        "    row = df_test.iloc[idx]\n",
        "    image_path = os.path.join(config.IMAGE_DIR, row['image_name'])\n",
        "\n",
        "    # Predict\n",
        "    result = predict_with_attention(\n",
        "        model, image_path, row['question'],\n",
        "        word2idx, idx2answer,\n",
        "        val_test_transform, config.DEVICE\n",
        "    )\n",
        "\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"  Image: {row['image_name']}\")\n",
        "    print(f\"  Question: {row['question']}\")\n",
        "    print(f\"  Answer Type: {row['answer_type']}\")\n",
        "    print(f\"  Image Organ: {row['image_organ']}\")\n",
        "    print(f\"  Question Type: {row['question_type']}\")\n",
        "    print(f\"  True answer: {row['answer']}\")\n",
        "    print(f\"  Predicted: {result['predicted_answer']}\")\n",
        "    correct = result['predicted_answer'] == row['answer']\n",
        "    print(f\"  Correct: {correct} {'‚úì' if correct else '‚úó'}\")\n",
        "    print(f\"  Top 5 predictions:\")\n",
        "    for ans, prob in result['top5_answers']:\n",
        "        print(f\"    {ans}: {prob:.4f}\")\n",
        "\n",
        "    # Visualize attention\n",
        "    visualize_attention(\n",
        "        result['image'],\n",
        "        result['attention_weights'],\n",
        "        row['question'],\n",
        "        result['predicted_answer'],\n",
        "        row['answer']\n",
        "    )\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "nXsTENb2QTZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"REFINED QUALITATIVE ANALYSIS: 5 OPEN & 5 CLOSED SAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Selection Logic: Filter for 5 Open and 5 Closed samples from the test set\n",
        "# We filter from df_test where the answer is actually \"answerable\" (seen in training)\n",
        "df_answerable_test = df_test[answerable_mask].copy()\n",
        "\n",
        "# Select 5 OPEN samples\n",
        "df_open = df_answerable_test[df_answerable_test['answer_type'] == 'OPEN'].head(5)\n",
        "\n",
        "# Select 5 CLOSED samples\n",
        "df_closed = df_answerable_test[df_answerable_test['answer_type'] == 'CLOSED'].head(5)\n",
        "\n",
        "# Combine them for the loop\n",
        "selected_samples = pd.concat([df_open, df_closed]).reset_index(drop=True)\n",
        "\n",
        "print(f\"Selected {len(selected_samples)} samples for visualization:\")\n",
        "print(f\" - {len(df_open)} OPEN-ended questions\")\n",
        "print(f\" - {len(df_closed)} CLOSED (Yes/No) questions\\n\")\n",
        "\n",
        "# 2. Iterative Prediction and Visualization Loop\n",
        "for i, row in selected_samples.iterrows():\n",
        "    image_path = os.path.join(config.IMAGE_DIR, row['image_name'])\n",
        "\n",
        "    # Run inference with attention\n",
        "    result = predict_with_attention(\n",
        "        model,\n",
        "        image_path,\n",
        "        row['question'],\n",
        "        word2idx,\n",
        "        idx2answer,\n",
        "        val_test_transform,\n",
        "        config.DEVICE\n",
        "    )\n",
        "\n",
        "    # Calculate correctness marker\n",
        "    is_correct = result['predicted_answer'] == row['answer']\n",
        "    marker = \"‚úì\" if is_correct else \"‚úó\"\n",
        "\n",
        "    # PRINT RESULTS IN YOUR SPECIFIC TEMPLATE\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Image: {row['image_name']}\")\n",
        "    print(f\"  Question: {row['question']}\")\n",
        "    print(f\"  Answer Type: {row['answer_type']}\")\n",
        "    print(f\"  Image Organ: {row['image_organ']}\")\n",
        "    print(f\"  Question Type: {row['question_type']}\")\n",
        "    print(f\"  True answer: {row['answer']}\")\n",
        "    print(f\"  Predicted: {result['predicted_answer']}\")\n",
        "    print(f\"  Correct: {is_correct} {marker}\")\n",
        "    print(f\"  Top 5 predictions:\")\n",
        "    for ans, prob in result['top5_answers']:\n",
        "        print(f\"    {ans}: {prob:.4f}\")\n",
        "\n",
        "    # 3. Call Visualization Function\n",
        "    visualize_attention(\n",
        "        result['image'],\n",
        "        result['attention_weights'],\n",
        "        row['question'],\n",
        "        result['predicted_answer'],\n",
        "        row['answer']\n",
        "    )\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n‚úì Refined Qualitative Analysis Complete\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "ifFSabjVSFlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_MTrJEkJl4x"
      },
      "source": [
        "---\n",
        "\n",
        "## üéâ Notebook Complete!\n",
        "\n",
        "**VQA-RAD Baseline - Methodologically Correct with Dual Reporting**\n",
        "\n",
        "**Configuration:**\n",
        "- ‚úÖ L2 Regularization (1e-5)\n",
        "- ‚úÖ Data Augmentation (conservative)\n",
        "- ‚úÖ Simplified Classifier\n",
        "- ‚úÖ Corrected Stacked Attention (progressive refinement)\n",
        "- ‚úÖ Weighted Loss + Stratification\n",
        "\n",
        "**Methodology:**\n",
        "- ‚úÖ Zero information leakage (partition first, vocab/labels from training only)\n",
        "- ‚úÖ Dual reporting: Full test set + Answerable subset\n",
        "- ‚úÖ Standard academic practice\n",
        "- ‚úÖ Comprehensive analysis\n",
        "\n",
        "**Evaluation Features:**\n",
        "- ‚úÖ Confusion matrix: Top 10 classes\n",
        "- ‚úÖ Prediction distribution analysis\n",
        "- ‚úÖ Unseen answers validation\n",
        "- ‚úÖ Confidence analysis (NEW)\n",
        "- ‚úÖ All metrics reported on both full test + answerable subset\n",
        "\n",
        "**Ready for academic submission!** üöÄ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "H8lWk5PiJl4P",
        "Y0y7JgvzJl4P",
        "aiKsV1ZUJl4R",
        "m6vNr4qZJl4X",
        "a_J2Ga1sJl4l",
        "CMEyvdq4Jl4n",
        "rMiPbdf8Jl4o",
        "llfSlHB5Jl4p",
        "A42jkGY4Jl4p",
        "Y2MJ0v3rJl4q",
        "WA97d5joJl4r",
        "PtoHlDArJl4r",
        "NbaGWDmwJl4s",
        "Oo3P9koJJl4t",
        "GrETmF-HJl4t",
        "1GGo6gTZJl4u",
        "AN6_DC7ZJl4v"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}