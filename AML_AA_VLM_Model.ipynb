{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChanZH0525/Adv-ML-QwenVLM/blob/main/AML_AA_VLM_Mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL2ti0W0wnQe"
      },
      "outputs": [],
      "source": [
        "#!pip install rouge-score\n",
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piDebM46w58H"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c09d1299"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering, BlipImageProcessor, AutoProcessor\n",
        "from transformers import BlipConfig\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrRYM8Z59YYA"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def set_seed(seed=49):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"âœ… All random seeds set to {seed}\")\n",
        "\n",
        "# Call it immediately\n",
        "set_seed(49)\n",
        "# ============================================\n",
        "\n",
        "# Then continue with device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V8_xWcBCvhK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# Upload zip file\n",
        "print(\"Upload your VQA-RAD images zip file:\")\n",
        "uploaded_zip = files.upload()\n",
        "\n",
        "# Upload CSV file\n",
        "print(\"\\nUpload your CSV file:\")\n",
        "uploaded_csv = files.upload()\n",
        "\n",
        "# Get filenames\n",
        "zip_filename = list(uploaded_zip.keys())[0]\n",
        "csv_filename = list(uploaded_csv.keys())[0]\n",
        "\n",
        "print(f\"\\nUploaded files:\")\n",
        "print(f\"  Zip: {zip_filename}\")\n",
        "print(f\"  CSV: {csv_filename}\")\n",
        "\n",
        "# Extract zip\n",
        "extract_to = 'vqa_rad_images/'\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"\\nImages extracted to: {extract_to}\")\n",
        "print(\"Extraction complete! âœ…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P0zDtMckuRr"
      },
      "outputs": [],
      "source": [
        "# Load CSV\n",
        "import os\n",
        "df = pd.read_csv(csv_filename)\n",
        "\n",
        "# Check structure\n",
        "print(f\"CSV loaded: {len(df)} rows\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nSample image names:\")\n",
        "print(df['image_name'].head())\n",
        "\n",
        "# Check extracted folder structure\n",
        "print(f\"\\nExtracted folder contents:\")\n",
        "for root, dirs, files in os.walk(extract_to):\n",
        "    print(f\"Folder: {root}\")\n",
        "    if files:\n",
        "        print(f\"Sample files: {files[:5]}\")\n",
        "    if dirs:\n",
        "        print(f\"Subfolders: {dirs}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNsOGdW_D_DV"
      },
      "outputs": [],
      "source": [
        "# Create full image paths\n",
        "df['full_image_path'] = df['image_name'].apply(lambda x: os.path.join(extract_to, x))\n",
        "\n",
        "# Verify images exist\n",
        "df['image_exists'] = df['full_image_path'].apply(os.path.exists)\n",
        "\n",
        "print(f\"Images found: {df['image_exists'].sum()} / {len(df)}\")\n",
        "print(f\"Missing images: {(~df['image_exists']).sum()}\")\n",
        "\n",
        "# Keep only valid samples\n",
        "df_valid = df[df['image_exists']].copy()\n",
        "\n",
        "print(f\"\\nValid dataset size: {len(df_valid)}\")\n",
        "print(f\"\\nQuestion types:\")\n",
        "print(df_valid['question_type'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UplQJeqAEnAT"
      },
      "outputs": [],
      "source": [
        "# Keep only essential columns\n",
        "columns_to_keep = [\n",
        "    'image_name',\n",
        "    'question',\n",
        "    'answer',\n",
        "    'question_type',\n",
        "    'answer_type',\n",
        "    'image_organ',\n",
        "    'split',\n",
        "    'full_image_path',\n",
        "    'image_exists'\n",
        "]\n",
        "\n",
        "df_valid = df_valid[columns_to_keep].copy()\n",
        "\n",
        "print(f\"Cleaned dataset columns: {df_valid.columns.tolist()}\")\n",
        "print(f\"Dataset size: {len(df_valid)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df_valid.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9HHYqWAFVhz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Use existing split column to separate the pool\n",
        "train_full = df_valid[df_valid['split'] == 'Train'].reset_index(drop=True)\n",
        "test_df = df_valid[df_valid['split'] == 'Test'].reset_index(drop=True)\n",
        "\n",
        "print(f\"Original splits from CSV:\")\n",
        "print(f\"Train Pool: {len(train_full)}\")\n",
        "print(f\"Test Set: {len(test_df)}\")\n",
        "\n",
        "# 2. Create stratified validation split from train_full (20%)\n",
        "# Added 'stratify' parameter here\n",
        "train_df, val_df = train_test_split(\n",
        "    train_full,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=train_full['answer_type']\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal splits (with Stratification):\")\n",
        "print(f\"Train: {len(train_df)}\")\n",
        "print(f\"Val:   {len(val_df)}\")\n",
        "print(f\"Test:  {len(test_df)}\")\n",
        "\n",
        "# Optional: Verify the stratification worked\n",
        "print(\"\\n--- Stratification Check (Answer Type Distribution) ---\")\n",
        "print(f\"Train Distribution:\\n{train_df['answer_type'].value_counts(normalize=True)}\")\n",
        "print(f\"Val Distribution:\\n{val_df['answer_type'].value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMpFC_jbXGtZ"
      },
      "outputs": [],
      "source": [
        "# Analyze answer lengths\n",
        "train_lengths = train_df['answer'].apply(lambda x: len(x.split())).describe()\n",
        "test_lengths = val_df['answer'].apply(lambda x: len(x.split())).describe()\n",
        "\n",
        "print(\"Training set answer lengths:\")\n",
        "print(train_lengths)\n",
        "print(\"\\nTest set answer lengths:\")\n",
        "print(test_lengths)\n",
        "\n",
        "# By answer type\n",
        "closed_lengths = df_valid[df_valid['answer_type']=='CLOSED']['answer'].apply(len)\n",
        "open_lengths = df_valid[df_valid['answer_type']=='OPEN']['answer'].apply(len)\n",
        "\n",
        "print(f\"\\nClosed: {closed_lengths.mean():.1f} words\")\n",
        "print(f\"Open: {open_lengths.mean():.1f} words\")\n",
        "print(f\"Open 95th percentile: {open_lengths.quantile(0.95):.0f} words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWf9unReC4mY"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define image preprocessing pipeline\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu4JRduSDQyg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load Qwen3-4B tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Add special image token\n",
        "special_tokens = {\"additional_special_tokens\": [\"<image>\"]}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "# Verify\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Image token ID: {tokenizer.convert_tokens_to_ids('<image>')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HavH8eWEDi-_"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, image_transform):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        image = Image.open(row['full_image_path']).convert('RGB')\n",
        "        pixel_values = self.image_transform(image)\n",
        "\n",
        "        # Construct strings\n",
        "        prompt = f\"<image>\\nQuestion: {row['question']}\\nAnswer: \"\n",
        "        full_text = prompt + f\"{row['answer']}{self.tokenizer.eos_token}\"\n",
        "\n",
        "        # Tokenize\n",
        "        full_encodings = self.tokenizer(full_text, truncation=True, max_length=128, padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "        input_ids = full_encodings['input_ids'].squeeze(0)\n",
        "        attention_mask = full_encodings['attention_mask'].squeeze(0)\n",
        "\n",
        "        # Create labels: NO QUESTION MASKING\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Only mask padding tokens (keep question tokens)\n",
        "        labels[attention_mask == 0] = -100\n",
        "\n",
        "        return {\n",
        "            'image': pixel_values,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "            'answer_text': row['answer'],\n",
        "            'question_type': row['question_type'],\n",
        "            'answer_type': row['answer_type'],\n",
        "            'image_organ': row['image_organ']\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "batch_size = 4\n",
        "train_dataset = VQADataset(train_df, tokenizer, image_transform)\n",
        "val_dataset = VQADataset(val_df, tokenizer, image_transform)\n",
        "test_dataset = VQADataset(test_df, tokenizer, image_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Datasets created WITHOUT answer-only loss masking:\")\n",
        "print(f\"âš ï¸  Model will now train on BOTH question and answer tokens\")\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Val: {len(val_dataset)} samples\")\n",
        "print(f\"Test: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrLJqWZWuUz2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import open_clip\n",
        "\n",
        "model, preprocess = open_clip.create_model_from_pretrained(\n",
        "    'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
        ")\n",
        "vision_encoder = model.visual  # Extract vision encoder only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Cdtrhkv0Ja"
      },
      "outputs": [],
      "source": [
        "# Freeze all parameters\n",
        "for param in vision_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Move to device\n",
        "vision_encoder = vision_encoder.to(device)\n",
        "vision_encoder.eval()\n",
        "\n",
        "# Verify\n",
        "print(f\"Vision encoder loaded\")\n",
        "print(f\"Parameters frozen: {not next(vision_encoder.parameters()).requires_grad}\")\n",
        "print(f\"Device: {next(vision_encoder.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy6aQ2XTvXGb"
      },
      "outputs": [],
      "source": [
        "# Check for the correct attribute\n",
        "if hasattr(vision_encoder, 'embed_dim'):\n",
        "    print(f\"Output dimension: {vision_encoder.embed_dim}\")\n",
        "elif hasattr(vision_encoder, 'num_features'):\n",
        "    print(f\"Output dimension: {vision_encoder.num_features}\")\n",
        "else:\n",
        "    # Test with dummy input\n",
        "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = vision_encoder(dummy_input)\n",
        "    print(f\"Output dimension: {output.shape[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW1Agm6MwH27"
      },
      "outputs": [],
      "source": [
        "# Get a sample from train dataset\n",
        "row_idx = 0 # Example row index\n",
        "sample = train_df.iloc[row_idx]\n",
        "image = Image.open(sample['full_image_path']).convert('RGB') # Load image using full_image_path\n",
        "\n",
        "# Apply preprocessing\n",
        "image_tensor = image_transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "# Extract features\n",
        "with torch.no_grad():\n",
        "    features = vision_encoder(image_tensor)\n",
        "\n",
        "print(f\"Input image shape: {image_tensor.shape}\")\n",
        "print(f\"Output features shape: {features.shape}\")\n",
        "print(f\"Feature sample (first 10 values): {features[0, :10]}\")\n",
        "\n",
        "# Visualize the image\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Q: {sample['question']}\\nA: {sample['answer']}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwePow4qwaiB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Projector(nn.Module):\n",
        "    def __init__(self, vision_dim=512, llm_dim=2560):\n",
        "        super().__init__()\n",
        "        # Wrap everything in nn.Sequential so PyTorch registers the parameters\n",
        "        self.proj = nn.Sequential(\n",
        "            # Stage 1: 512 -> 1024\n",
        "            nn.Linear(vision_dim, 1024),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # Stage 2: 1024 -> 2048\n",
        "            nn.Linear(1024, 2048),\n",
        "            nn.LayerNorm(2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            # Stage 3: 2048 -> 2560 (LLM Hidden Size)\n",
        "            nn.Linear(2048, llm_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Now self.proj is defined and contains all layers\n",
        "        return self.proj(x)\n",
        "\n",
        "# Create projector\n",
        "# Ensure 'device' is defined (e.g., device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "projector = Projector(vision_dim=512, llm_dim=2560).to(device)\n",
        "\n",
        "print(f\"Projector created\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in projector.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gClOzhMwwg1O"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 8-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "# Load LLM\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "llm = get_peft_model(llm, lora_config)\n",
        "\n",
        "print(\"LLM loaded with LoRA\")\n",
        "llm.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYw1mU3szK0K"
      },
      "outputs": [],
      "source": [
        "class VLMModel(nn.Module):\n",
        "    def __init__(self, vision_encoder, projector, llm, tokenizer):\n",
        "        super().__init__()\n",
        "        self.vision_encoder = vision_encoder\n",
        "        self.projector = projector\n",
        "        self.llm = llm\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_token_id = tokenizer.convert_tokens_to_ids('<image>')\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask, labels=None):\n",
        "        # Extract visual features\n",
        "        with torch.no_grad():\n",
        "            visual_features = self.vision_encoder(images)  # [batch, 512]\n",
        "\n",
        "        # Project to LLM embedding space\n",
        "        visual_embeds = self.projector(visual_features)\n",
        "        visual_embeds = visual_embeds.unsqueeze(1)\n",
        "\n",
        "        # Get text embeddings\n",
        "        text_embeds = self.llm.get_input_embeddings()(input_ids)\n",
        "\n",
        "        # Find <image> token positions and replace with visual embeddings\n",
        "        batch_size = input_ids.shape[0]\n",
        "        for i in range(batch_size):\n",
        "            image_pos = (input_ids[i] == self.image_token_id).nonzero(as_tuple=True)[0]\n",
        "            if len(image_pos) > 0:\n",
        "                text_embeds[i, image_pos[0]] = visual_embeds[i, 0]\n",
        "\n",
        "        # Forward through LLM\n",
        "        outputs = self.llm(\n",
        "            inputs_embeds=text_embeds,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Compute loss on ALL tokens (question + answer)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Shift for causal LM: predict next token\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            # Only ignore padding tokens (-100), train on everything else\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                          shift_labels.view(-1))\n",
        "\n",
        "            # Create outputs with loss\n",
        "            from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "            outputs = CausalLMOutputWithPast(loss=loss, logits=logits)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Recreate VLM without answer-only masking\n",
        "vlm_model = VLMModel(vision_encoder, projector, llm, tokenizer)\n",
        "print(\"VLM model created WITHOUT answer-only loss\")\n",
        "print(\"âš ï¸  Model will train on BOTH question and answer tokens\")\n",
        "print(\"Note: Only padding tokens are masked in the dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_5rbVWJ8yFc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from peft import get_peft_model_state_dict, set_peft_model_state_dict\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=2, min_delta=0, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        '''Saves ONLY the trainable parts of the model.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased. Saving best model state...')\n",
        "\n",
        "        # 1. Capture Projector state\n",
        "        projector_state = copy.deepcopy(model.projector.state_dict())\n",
        "\n",
        "        # 2. Capture ONLY the LoRA adapter weights (NOT the whole LLM)\n",
        "        lora_state = get_peft_model_state_dict(model.llm)\n",
        "\n",
        "        self.best_state = {\n",
        "            'projector': projector_state,\n",
        "            'llm_lora': lora_state\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ie9PIwUmkqA"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_lldYjzzRW8"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Get trainable parameters (projector + LoRA)\n",
        "trainable_params = [\n",
        "    {'params': projector.parameters(), 'lr': 0.0001},\n",
        "    {'params': llm.parameters(), 'lr': 0.00005}\n",
        "]\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = AdamW(trainable_params, weight_decay=0.01)\n",
        "\n",
        "# Define gradient accumulation steps before usage\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Calculate total training steps\n",
        "steps_per_epoch = len(train_loader) // gradient_accumulation_steps\n",
        "num_epochs = 20\n",
        "warmup_steps = 200\n",
        "total_steps = num_epochs * steps_per_epoch\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n",
        "\n",
        "# Warmup + Cosine Annealing Scheduler\n",
        "def lr_lambda(current_step):\n",
        "    if current_step < warmup_steps:\n",
        "        # Linear warmup\n",
        "        return float(current_step) / float(max(1, warmup_steps))\n",
        "    # Cosine annealing after warmup\n",
        "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "scheduler = LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Early stopping config\n",
        "patience = 2\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "print(f\"  Max epochs: {num_epochs}\")\n",
        "print(f\"  Early stopping patience: {patience}\")\n",
        "print(f\"  Effective batch size: {batch_size * gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQB6rOvNDpku"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, device, gradient_accumulation_steps):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        # Move batch to device\n",
        "        images = batch['image'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, input_ids, attention_mask, labels)\n",
        "        loss = outputs.loss / gradient_accumulation_steps\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights every gradient_accumulation_steps\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # â† Step-level LR update\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "        # Show loss and current LR\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',\n",
        "            'lr': f'{current_lr:.2e}'\n",
        "        })\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5sH4P26EUMw"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(val_loader, desc=\"Validation\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            images = batch['image'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, input_ids, attention_mask, labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "print(\"Validation function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljKyh4UlEu5g"
      },
      "outputs": [],
      "source": [
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting training with Early Stopping...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*50}\\nEpoch {epoch + 1}/{num_epochs}\\n{'='*50}\")\n",
        "\n",
        "    # 1. Train (now passes scheduler)\n",
        "    train_loss = train_epoch(vlm_model, train_loader, optimizer, scheduler, device, gradient_accumulation_steps)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 2. Validate\n",
        "    val_loss = validate(vlm_model, val_loader, device)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Get current learning rate\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
        "    print(f\"  Train-Val Gap: {val_loss - train_loss:.4f}\")\n",
        "\n",
        "    # 3. Early Stopping Check\n",
        "    early_stopping(val_loss, vlm_model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered. Training halted.\")\n",
        "        break\n",
        "# After the training loop breaks\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# âœ… CRITICAL FIX: Restore best model weights\n",
        "if early_stopping.best_state:\n",
        "    best_epoch = np.argmin(val_losses) + 1\n",
        "    print(f\"\\nRestoring best model from Epoch {best_epoch} (Val Loss: {min(val_losses):.4f})\")\n",
        "\n",
        "    vlm_model.projector.load_state_dict(early_stopping.best_state['projector'])\n",
        "    set_peft_model_state_dict(vlm_model.llm, early_stopping.best_state['llm_lora'])\n",
        "\n",
        "    print(\"âœ… Best model weights restored successfully!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Warning: No best state saved, using final epoch weights\")\n",
        "\n",
        "# Now continue with evaluation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDp9vuAkFlW9"
      },
      "outputs": [],
      "source": [
        "def generate_answer(model, image, question, tokenizer, device, answer_type='OPEN', max_length=15):\n",
        "    model.eval()\n",
        "\n",
        "    image_tensor = image_transform(image).unsqueeze(0).to(device)\n",
        "    prompt = f\"<image>\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Answer-type-specific token limits\n",
        "    if answer_type == 'CLOSED':\n",
        "        max_new_tokens = 3\n",
        "        min_new_tokens = 1\n",
        "    else:  # OPEN\n",
        "        max_new_tokens = 25\n",
        "        min_new_tokens = 2\n",
        "\n",
        "    with torch.no_grad():\n",
        "        visual_features = model.vision_encoder(image_tensor)\n",
        "        visual_embeds = model.projector(visual_features).unsqueeze(1)\n",
        "\n",
        "        text_embeds = model.llm.get_input_embeddings()(input_ids)\n",
        "        image_pos = (input_ids[0] == model.image_token_id).nonzero(as_tuple=True)[0]\n",
        "        if len(image_pos) > 0:\n",
        "            text_embeds[0, image_pos[0]] = visual_embeds[0, 0]\n",
        "\n",
        "        outputs = model.llm.generate(\n",
        "            inputs_embeds=text_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            min_new_tokens=min_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.2\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # Post-processing based on answer type\n",
        "    if answer_type == 'CLOSED':\n",
        "        # For closed: extract only yes/no, truncate aggressively\n",
        "        answer = answer.split('.')[0].split('\\n')[0].split(',')[0].strip()\n",
        "    else:\n",
        "        # For open: allow full sentence but remove trailing artifacts\n",
        "        answer = answer.split('\\n')[0].strip()\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBteUVztQOC8"
      },
      "outputs": [],
      "source": [
        "def visualize_test_sample(model, test_df, tokenizer, device, sample_idx=None,\n",
        "                          random_sample=False, answer_type=None):\n",
        "    \"\"\"\n",
        "    Visualize and evaluate a test sample with optional answer type filtering.\n",
        "\n",
        "    Args:\n",
        "        model: VLM model\n",
        "        test_df: Test DataFrame\n",
        "        tokenizer: Tokenizer\n",
        "        device: torch device\n",
        "        sample_idx: Specific index to visualize (None for first)\n",
        "        random_sample: If True, pick random sample\n",
        "        answer_type: Filter by 'CLOSED' or 'OPEN' (None for no filter)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prediction details\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Filter by answer type if specified\n",
        "    if answer_type is not None:\n",
        "        filtered_df = test_df[test_df['answer_type'] == answer_type].reset_index(drop=True)\n",
        "        if len(filtered_df) == 0:\n",
        "            print(f\"âŒ No samples found with answer_type='{answer_type}'\")\n",
        "            return None\n",
        "        df_to_use = filtered_df\n",
        "    else:\n",
        "        df_to_use = test_df\n",
        "\n",
        "    # Determine which sample to use\n",
        "    if random_sample:\n",
        "        sample_idx = random.randint(0, len(df_to_use) - 1)\n",
        "    elif sample_idx is None:\n",
        "        sample_idx = 0  # Default to first sample\n",
        "\n",
        "    # Get sample\n",
        "    row = df_to_use.iloc[sample_idx]\n",
        "    image = Image.open(row['full_image_path']).convert('RGB')\n",
        "\n",
        "    # Generate prediction\n",
        "    predicted_answer = generate_answer(\n",
        "        model,\n",
        "        image,\n",
        "        row['question'],\n",
        "        tokenizer,\n",
        "        device,\n",
        "        answer_type=row['answer_type']  # Pass answer type to generation\n",
        "    )\n",
        "\n",
        "    # Display image\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Question: {row['question']}\", fontsize=12, wrap=True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print details\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Sample Index: {sample_idx} (from {answer_type if answer_type else 'ALL'} samples)\")\n",
        "    print(f\"Question Type: {row['question_type']}\")\n",
        "    print(f\"Answer Type: {row['answer_type']}\")\n",
        "    print(f\"Image Organ: {row['image_organ']}\")\n",
        "    print(f\"\\nQuestion: {row['question']}\")\n",
        "    print(f\"Ground Truth: {row['answer']}\")\n",
        "    print(f\"Predicted: {predicted_answer}\")\n",
        "\n",
        "    # Check if correct\n",
        "    gt_lower = row['answer'].lower().strip()\n",
        "    pred_lower = predicted_answer.lower().strip()\n",
        "\n",
        "    # Special handling for closed-ended\n",
        "    if row['answer_type'] == 'CLOSED':\n",
        "        gt_clean = clean_closed_answer(gt_lower)\n",
        "        pred_clean = clean_closed_answer(pred_lower)\n",
        "        correct = gt_clean == pred_clean\n",
        "        if correct:\n",
        "            print(f\"\\nâœ… EXACT MATCH! ('{pred_clean}' == '{gt_clean}')\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ INCORRECT (predicted '{pred_clean}', expected '{gt_clean}')\")\n",
        "    else:\n",
        "        # Open-ended exact match\n",
        "        if gt_lower == pred_lower:\n",
        "            print(f\"\\nâœ… EXACT MATCH!\")\n",
        "        elif pred_lower in gt_lower or gt_lower in pred_lower:\n",
        "            print(f\"\\nâš ï¸  PARTIAL MATCH\")\n",
        "        else:\n",
        "            print(f\"\\nâŒ INCORRECT\")\n",
        "        correct = gt_lower == pred_lower\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Return details\n",
        "    return {\n",
        "        'index': sample_idx,\n",
        "        'question': row['question'],\n",
        "        'ground_truth': row['answer'],\n",
        "        'prediction': predicted_answer,\n",
        "        'question_type': row['question_type'],\n",
        "        'answer_type': row['answer_type'],\n",
        "        'correct': correct,\n",
        "        'image_organ': row['image_organ']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfKwzf-IhhLW"
      },
      "outputs": [],
      "source": [
        "# After training completes, calculate test loss\n",
        "def compute_test_loss(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Computing Test Loss\"):\n",
        "            images = batch['image'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(images, input_ids, attention_mask, labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "# Calculate test loss\n",
        "test_loss = compute_test_loss(vlm_model, test_loader, device)\n",
        "\n",
        "print(f\"\\nFinal Loss Comparison:\")\n",
        "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\n",
        "#print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Plot all three\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Training and validation over epochs\n",
        "epochs = range(1, len(train_losses) + 1) # Adjusted to match the actual number of completed epochs\n",
        "plt.plot(epochs, train_losses, marker='o', label='Training Loss', linewidth=2)\n",
        "plt.plot(epochs, val_losses, marker='s', label='Validation Loss', linewidth=2)\n",
        "\n",
        "# Test loss as horizontal line (computed once)\n",
        "#plt.axhline(y=test_loss, color='red', linestyle='--', linewidth=2, label='Test Loss')\n",
        "\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Training, Validation Loss', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKkvoGVaTlwn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# Remove the nltk.data import\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "#from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh18KrG_U2_q"
      },
      "outputs": [],
      "source": [
        "def clean_closed_answer(pred):\n",
        "    \"\"\"\n",
        "    Extract yes/no from potentially verbose answers.\n",
        "    Handles cases like \"Yes, there is...\", \"No signs of...\", etc.\n",
        "    \"\"\"\n",
        "    pred = pred.lower().strip()\n",
        "\n",
        "    # Match \"yes\" at start of string (with word boundary)\n",
        "    if re.match(r'^yes\\b', pred):\n",
        "        return \"yes\"\n",
        "\n",
        "    # Match \"no\" at start of string (with word boundary)\n",
        "    if re.match(r'^no\\b', pred):\n",
        "        return \"no\"\n",
        "\n",
        "    # If no clear yes/no pattern, return first word as fallback\n",
        "    return pred.split()[0] if pred.split() else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49sDoHt9jXMQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def clean_closed_answer(pred):\n",
        "    \"\"\"Extract yes/no from potentially verbose answers.\"\"\"\n",
        "    pred = pred.lower().strip()\n",
        "    if re.match(r'^yes\\b', pred):\n",
        "        return \"yes\"\n",
        "    if re.match(r'^no\\b', pred):\n",
        "        return \"no\"\n",
        "    return pred.split()[0] if pred.split() else \"\"\n",
        "\n",
        "\n",
        "def compute_wbss(pred_text, gt_text):\n",
        "    \"\"\"Compute Wu-Palmer Similarity Score between two texts.\"\"\"\n",
        "    pred_words = pred_text.lower().split()\n",
        "    gt_words = gt_text.lower().split()\n",
        "\n",
        "    if not pred_words or not gt_words:\n",
        "        return 0.0\n",
        "\n",
        "    similarities = []\n",
        "    for pred_word in pred_words:\n",
        "        pred_synsets = wn.synsets(pred_word)\n",
        "        if not pred_synsets:\n",
        "            continue\n",
        "\n",
        "        max_sim = 0.0\n",
        "        for gt_word in gt_words:\n",
        "            gt_synsets = wn.synsets(gt_word)\n",
        "            if not gt_synsets:\n",
        "                continue\n",
        "\n",
        "            for ps in pred_synsets:\n",
        "                for gs in gt_synsets:\n",
        "                    sim = ps.wup_similarity(gs)\n",
        "                    if sim and sim > max_sim:\n",
        "                        max_sim = sim\n",
        "\n",
        "        if max_sim > 0:\n",
        "            similarities.append(max_sim)\n",
        "\n",
        "    return np.mean(similarities) if similarities else 0.0\n",
        "\n",
        "\n",
        "def evaluate_comprehensive(model, test_df, tokenizer, device):\n",
        "    \"\"\"Simplified comprehensive evaluation with answer-type-specific generation.\"\"\"\n",
        "\n",
        "    # Generate all predictions\n",
        "    print(\"Generating predictions...\")\n",
        "    predictions = []\n",
        "\n",
        "    for idx in tqdm(range(len(test_df)), desc=\"Evaluating\"):\n",
        "        row = test_df.iloc[idx]\n",
        "        image = Image.open(row['full_image_path']).convert('RGB')\n",
        "\n",
        "        pred = generate_answer(model, image, row['question'], tokenizer, device,\n",
        "                              answer_type=row['answer_type'])\n",
        "\n",
        "        predictions.append({\n",
        "            'prediction': pred.lower().strip(),\n",
        "            'ground_truth': row['answer'].lower().strip(),\n",
        "            'question_type': row['question_type'],\n",
        "            'answer_type': row['answer_type']\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(predictions)\n",
        "\n",
        "    # Compute correctness for each row\n",
        "    def is_correct(row):\n",
        "        if row['answer_type'] == 'CLOSED':\n",
        "            return clean_closed_answer(row['prediction']) == clean_closed_answer(row['ground_truth'])\n",
        "        else:\n",
        "            return row['prediction'] == row['ground_truth']\n",
        "\n",
        "    df['correct'] = df.apply(is_correct, axis=1)\n",
        "\n",
        "    # Split by answer type\n",
        "    closed_df = df[df['answer_type'] == 'CLOSED']\n",
        "    open_df = df[df['answer_type'] == 'OPEN']\n",
        "\n",
        "    # === CLOSED-ENDED METRICS ===\n",
        "    closed_acc = (closed_df['correct'].sum() / len(closed_df) * 100) if len(closed_df) > 0 else 0\n",
        "\n",
        "    # === OPEN-ENDED METRICS ===\n",
        "    print(\"\\nðŸ“Š Computing open-ended metrics...\")\n",
        "\n",
        "    open_metrics = {\n",
        "        'exact': [],\n",
        "        'bleu1': [],\n",
        "        'bleu2': [],\n",
        "        'bleu3': [],\n",
        "        'bleu4': [],\n",
        "        'wbss': []\n",
        "    }\n",
        "\n",
        "    smoothing = SmoothingFunction()\n",
        "\n",
        "    for _, row in tqdm(open_df.iterrows(), total=len(open_df), desc=\"BLEU/WBSS\"):\n",
        "        pred_tokens = row['prediction'].split()\n",
        "        gt_tokens = row['ground_truth'].split()\n",
        "\n",
        "        # Exact match\n",
        "        open_metrics['exact'].append(1.0 if row['correct'] else 0.0)\n",
        "\n",
        "        # BLEU scores\n",
        "        if pred_tokens and gt_tokens:\n",
        "            open_metrics['bleu1'].append(sentence_bleu([gt_tokens], pred_tokens,\n",
        "                                                       weights=(1, 0, 0, 0),\n",
        "                                                       smoothing_function=smoothing.method1))\n",
        "            open_metrics['bleu2'].append(sentence_bleu([gt_tokens], pred_tokens,\n",
        "                                                       weights=(0.5, 0.5, 0, 0),\n",
        "                                                       smoothing_function=smoothing.method1))\n",
        "            open_metrics['bleu3'].append(sentence_bleu([gt_tokens], pred_tokens,\n",
        "                                                       weights=(0.33, 0.33, 0.33, 0),\n",
        "                                                       smoothing_function=smoothing.method1))\n",
        "            open_metrics['bleu4'].append(sentence_bleu([gt_tokens], pred_tokens,\n",
        "                                                       weights=(0.25, 0.25, 0.25, 0.25),\n",
        "                                                       smoothing_function=smoothing.method1))\n",
        "        else:\n",
        "            open_metrics['bleu1'].append(0.0)\n",
        "            open_metrics['bleu2'].append(0.0)\n",
        "            open_metrics['bleu3'].append(0.0)\n",
        "            open_metrics['bleu4'].append(0.0)\n",
        "\n",
        "        # WBSS\n",
        "        open_metrics['wbss'].append(compute_wbss(row['prediction'], row['ground_truth']))\n",
        "\n",
        "    # Average open-ended metrics\n",
        "    open_results = {k: np.mean(v) * 100 if v else 0 for k, v in open_metrics.items()}\n",
        "\n",
        "    # === PRINT RESULTS ===\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \" * 20 + \"EVALUATION RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\nðŸ“Š CLOSED-ENDED PERFORMANCE:\")\n",
        "    print(f\"   Total: {len(closed_df)}\")\n",
        "    print(f\"   Accuracy: {closed_acc:.2f}% ({closed_df['correct'].sum()}/{len(closed_df)})\")\n",
        "\n",
        "    print(f\"\\nðŸ“Š OPEN-ENDED PERFORMANCE:\")\n",
        "    print(f\"   Total: {len(open_df)}\")\n",
        "    print(f\"   Exact Match:  {open_results['exact']:.2f}%\")\n",
        "    print(f\"   BLEU-1:       {open_results['bleu1']:.2f}%\")\n",
        "    print(f\"   BLEU-2:       {open_results['bleu2']:.2f}%\")\n",
        "    print(f\"   BLEU-3:       {open_results['bleu3']:.2f}%\")\n",
        "    print(f\"   BLEU-4:       {open_results['bleu4']:.2f}%\")\n",
        "    print(f\"   WBSS:         {open_results['wbss']:.2f}%\")\n",
        "\n",
        "    print(f\"\\nðŸ“Š PERFORMANCE BY QUESTION TYPE:\")\n",
        "    for qtype in sorted(df['question_type'].unique()):\n",
        "        subset = df[df['question_type'] == qtype]\n",
        "        acc = subset['correct'].sum() / len(subset) * 100\n",
        "        print(f\"   {qtype:15s}: {acc:5.2f}% ({subset['correct'].sum()}/{len(subset)})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        'closed_acc': closed_acc,\n",
        "        'closed_correct': closed_df['correct'].sum(),\n",
        "        'closed_total': len(closed_df),\n",
        "        'open_exact': open_results['exact'],\n",
        "        'open_bleu1': open_results['bleu1'],\n",
        "        'open_bleu2': open_results['bleu2'],\n",
        "        'open_bleu3': open_results['bleu3'],\n",
        "        'open_bleu4': open_results['bleu4'],\n",
        "        'open_wbss': open_results['wbss'],\n",
        "        'open_total': len(open_df),\n",
        "        'df_results': df\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHTtThGITgc1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# Run complete evaluation with improved regex cleaning\n",
        "metrics = evaluate_comprehensive(vlm_model, test_df, tokenizer, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get first 5 closed-ended samples\n",
        "print(\"=\"*70)\n",
        "print(\"FIRST 5 CLOSED-ENDED SAMPLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(272):\n",
        "    result = visualize_test_sample(\n",
        "        vlm_model,\n",
        "        test_df,\n",
        "        tokenizer,\n",
        "        device,\n",
        "        sample_idx=i,\n",
        "        answer_type='CLOSED'\n",
        "    )\n",
        "    print(\"\\n\")  # Extra spacing between samples"
      ],
      "metadata": {
        "id": "6FnM16Vw5Z44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(179):\n",
        "    result = visualize_test_sample(\n",
        "        vlm_model,\n",
        "        test_df,\n",
        "        tokenizer,\n",
        "        device,\n",
        "        sample_idx=i,\n",
        "        answer_type='OPEN'\n",
        "    )\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "ciL6gWbOmrxz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
